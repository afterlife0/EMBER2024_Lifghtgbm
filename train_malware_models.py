#!/usr/bin/env python3
"""
EMBER2024 Malware Detection Model Training Script
Supports LightGBM, XGBoost, and Random Forest with comprehensive argument parsing

Features:
- Multiple model selection (LightGBM, XGBoost, Random Forest, or any combination)
- Dataset category selection (Win32, Win64, APK, PDF, ELF, Net)
- Custom data size sampling
- Hyperparameter configuration
- Cross-validation and evaluation
- Model saving and metrics export
- Training curves and accuracy visualizations

Usage Examples:
python train_malware_models.py --category Win32 --models lightgbm xgboost randomforest --sample_size 100000
python train_malware_models.py --category APK --models lightgbm --lgb_n_estimators 500 --cv_folds 3
python train_malware_models.py --category all --models xgboost randomforest --xgb_max_depth 8 --sample_size 50000
"""

import argparse
import pandas as pd
import numpy as np
import os
import time
import json
import glob
import warnings
import pyarrow.parquet as pq
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

# Machine Learning Libraries
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report,
    precision_recall_curve, auc, average_precision_score,
    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score,
    log_loss, brier_score_loss, roc_curve
)
from sklearn.preprocessing import LabelEncoder
import joblib

# Visualization Libraries
import matplotlib.pyplot as plt
import seaborn as sns
try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    import plotly.offline as pyo
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("Plotly not available - some advanced visualizations will be disabled")

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

class MalwareModelTrainer:
    """
    Comprehensive malware detection model trainer for EMBER2024 dataset
    """
    
    def __init__(self, args):
        self.args = args
        self.data_dir = Path(args.data_dir)
        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Available categories and their file patterns for split files
        self.categories = {
            'Win32': '*Win32_train_ultra_focused.parquet',
            'Win64': '*Win64_train_ultra_focused.parquet',
            'APK': '*APK_train_ultra_focused.parquet',
            'PDF': '*PDF_train_ultra_focused.parquet',
            'ELF': '*ELF_train_ultra_focused.parquet',
            'Net': '*Dot_Net_train_ultra_focused.parquet'  # Note: Dot_Net in file names
        }
        
        # Test file patterns (if they exist - currently only train files available)
        self.test_categories = {
            'Win32': '*Win32_test_ultra_focused.parquet',
            'Win64': '*Win64_test_ultra_focused.parquet',
            'APK': '*APK_test_ultra_focused.parquet',
            'PDF': '*PDF_test_ultra_focused.parquet',
            'ELF': '*ELF_test_ultra_focused.parquet',
            'Net': '*Dot_Net_test_ultra_focused.parquet'
        }
        
        self.results = {}
        
        # Define smart sampling for "all" category
        self.smart_all_sampling = {
            'Win32': 0.45,    # 45% - largest dataset
            'Win64': 0.22,    # 22% - second largest
            'APK': 0.15,     # 15% - android focus
            'Net': 0.15,     # 15% - .NET focus
            'PDF': 0.08,     # 8% - smaller but important
            'ELF': 0.05      # 5% - unix/linux
        }
        
        # Visualization settings
        self.viz_dir = self.output_dir / "visualizations"
        self.viz_dir.mkdir(parents=True, exist_ok=True)
        
        # Set visualization style
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # Challenge set configuration
        self.challenge_set_pattern = '*challenge_malicious_ultra_focused.parquet'
        
    def load_data(self, category: str) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:
        """Load training and test data for a specific category from split files - sequential loading"""
        print(f"\nðŸ“Š Loading {category} dataset from split files...")
        
        # Find all training files for this category
        train_pattern = self.data_dir / self.categories[category]
        train_files = glob.glob(str(train_pattern))
        
        if not train_files:
            raise FileNotFoundError(f"No training files found for {category} with pattern: {train_pattern}")
        
        print(f"   Found {len(train_files)} training file(s) for {category}")
        
        # Sequential loading approach - load files until we have enough samples
        train_dfs = []
        total_samples = 0
        files_loaded = 0
        target_samples = self.args.sample_size if self.args.sample_size else float('inf')
        
        for train_file in sorted(train_files):
            if total_samples >= target_samples:
                break
                
            file_path = Path(train_file)
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            
            try:
                # Load file
                df = pd.read_parquet(train_file)
                
                # If this file would exceed our target, take only what we need
                remaining_needed = target_samples - total_samples
                if len(df) > remaining_needed:
                    df = df.head(int(remaining_needed))
                    print(f"     âœ… {file_path.name}: {len(df):,} samples (partial, {file_size_mb:.1f} MB)")
                else:
                    print(f"     âœ… {file_path.name}: {len(df):,} samples ({file_size_mb:.1f} MB)")
                
                train_dfs.append(df)
                total_samples += len(df)
                files_loaded += 1
                
                # Break if we've reached our target
                if total_samples >= target_samples:
                    break
                
            except Exception as e:
                print(f"     âŒ Error loading {file_path.name}: {e}")
                continue
        
        if not train_dfs:
            raise ValueError(f"No training data could be loaded for {category}")
        
        # Combine only the loaded training data
        train_df = pd.concat(train_dfs, ignore_index=True)
        print(f"   ðŸ“Š Loaded training data: {total_samples:,} samples from {files_loaded} files")
        
        # Look for test files (if they exist) - sequential loading
        test_df = None
        test_pattern = self.data_dir / self.test_categories[category]
        test_files = glob.glob(str(test_pattern))
        
        if test_files:
            print(f"   Found {len(test_files)} test file(s) for {category}")
            test_dfs = []
            total_test_samples = 0
            test_files_loaded = 0
            test_target_samples = self.args.sample_size if self.args.sample_size else float('inf')
            
            for test_file in sorted(test_files):
                if total_test_samples >= test_target_samples:
                    break
                    
                try:
                    # Load test file
                    df = pd.read_parquet(test_file)
                    
                    # If this file would exceed our target, take only what we need
                    remaining_needed = test_target_samples - total_test_samples
                    if len(df) > remaining_needed:
                        df = df.sample(n=int(remaining_needed), random_state=42).reset_index(drop=True)
                        print(f"     âœ… {Path(test_file).name}: {len(df):,} samples (partial)")
                    else:
                        print(f"     âœ… {Path(test_file).name}: {len(df):,} samples")
                    
                    test_dfs.append(df)
                    total_test_samples += len(df)
                    test_files_loaded += 1
                    
                    # Break if we've reached our target
                    if total_test_samples >= test_target_samples:
                        break
                        
                except Exception as e:
                    print(f"     âŒ Error loading {Path(test_file).name}: {e}")
                    continue
            
            if test_dfs:
                test_df = pd.concat(test_dfs, ignore_index=True)
                print(f"   ðŸ“Š Loaded test data: {total_test_samples:,} samples from {test_files_loaded} files")
        else:
            print(f"   â„¹ï¸  No test files found for {category}")
        
        return train_df, test_df
    
    def load_challenge_set(self) -> Optional[pd.DataFrame]:
        """Load challenge set for validation when training on all categories"""
        challenge_pattern = self.data_dir / self.challenge_set_pattern
        challenge_files = glob.glob(str(challenge_pattern))
        
        if not challenge_files:
            print(f"   âš ï¸  No challenge files found with pattern: {challenge_pattern}")
            return None
        
        try:
            print(f"\nðŸŽ¯ Loading challenge set for validation...")
            print(f"   Found {len(challenge_files)} challenge file(s)")
            
            challenge_dfs = []
            total_samples = 0
            
            for challenge_file in sorted(challenge_files):
                file_path = Path(challenge_file)
                df = pd.read_parquet(challenge_file)
                challenge_dfs.append(df)
                total_samples += len(df)
                print(f"     âœ… {file_path.name}: {len(df):,} samples")
            
            combined_challenge_df = pd.concat(challenge_dfs, ignore_index=True)
            print(f"   Challenge set loaded: {total_samples:,} samples, {combined_challenge_df.shape[1]:,} features")
            
            # Show challenge set statistics
            if 'label' in combined_challenge_df.columns:
                label_dist = combined_challenge_df['label'].value_counts().to_dict()
                print(f"   Challenge set label distribution: {label_dist}")
                
                # Check if challenge set has balanced classes
                unique_labels = set(label_dist.keys())
                if len(unique_labels) == 1:
                    print(f"   âš ï¸  Challenge set contains only one class (label={list(unique_labels)[0]})")
                    print(f"   ðŸ’¡ Will create balanced validation set using challenge samples + benign samples")
                    return combined_challenge_df
                else:
                    print(f"   âœ… Challenge set contains multiple classes - ready for validation")
            
            return combined_challenge_df
            
        except Exception as e:
            print(f"   âŒ Error loading challenge set: {str(e)}")
            return None
    
    def create_balanced_challenge_validation(self, train_df: pd.DataFrame, challenge_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """Create balanced validation set using challenge samples and benign samples from training data"""
        print(f"   ðŸ”„ Creating balanced validation set from challenge + training data...")
        
        # Get challenge samples (malicious)
        X_challenge, y_challenge = self.prepare_features(challenge_df)
        challenge_samples = len(y_challenge)
        
        # Calculate number of benign samples based on ratio
        target_benign_samples = int(challenge_samples * self.args.challenge_balance_ratio)
        
        # Extract benign samples from training data
        train_benign = train_df[train_df['label'] == 0].copy()
        
        if len(train_benign) >= target_benign_samples:
            # Sample benign data to match target
            train_benign_sampled = train_benign.sample(n=target_benign_samples, random_state=42)
            print(f"   ðŸ“Š Sampled {target_benign_samples:,} benign samples from training data (ratio: {self.args.challenge_balance_ratio})")
        else:
            # Use all available benign samples
            train_benign_sampled = train_benign
            actual_ratio = len(train_benign_sampled) / challenge_samples
            print(f"   ðŸ“Š Using all {len(train_benign):,} available benign samples (actual ratio: {actual_ratio:.2f})")
        
        # Prepare benign validation features
        X_benign_val, y_benign_val = self.prepare_features(train_benign_sampled)
        
        # Combine challenge (malicious) and benign samples for validation
        X_val = pd.concat([X_challenge, X_benign_val], ignore_index=True)
        y_val = pd.concat([y_challenge, y_benign_val], ignore_index=True)
        
        # Shuffle validation set
        val_indices = np.random.RandomState(42).permutation(len(X_val))
        X_val = X_val.iloc[val_indices].reset_index(drop=True)
        y_val = y_val.iloc[val_indices].reset_index(drop=True)
        
        # Remove sampled benign data from training set
        remaining_train_df = train_df[~train_df.index.isin(train_benign_sampled.index)].copy()
        X_train, y_train = self.prepare_features(remaining_train_df)
        
        print(f"   âœ… Balanced validation set created:")
        print(f"      - Malicious samples: {challenge_samples:,} (from challenge set)")
        print(f"      - Benign samples: {len(y_benign_val):,} (from training data)")
        print(f"      - Total validation: {len(y_val):,} samples")
        print(f"      - Remaining training: {len(y_train):,} samples")
        print(f"      - Validation label distribution: {dict(y_val.value_counts())}")
        
        return X_train, X_val, y_train, y_val
    
    def load_combined_data(self, categories: List[str], total_sample_size: Optional[int] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:
        """Load and combine data from multiple categories intelligently"""
        print(f"\nðŸ“Š Loading combined dataset from {len(categories)} categories...")
        
        train_dfs = []
        test_dfs = []
        
        # Calculate samples per category
        if total_sample_size:
            samples_per_category = {}
            for category in categories:
                if category in self.smart_all_sampling:
                    samples_per_category[category] = int(total_sample_size * self.smart_all_sampling[category])
                else:
                    # Equal distribution for unlisted categories
                    samples_per_category[category] = total_sample_size // len(categories)
            
            print(f"   Smart sampling distribution:")
            for cat, samples in samples_per_category.items():
                print(f"     {cat}: {samples:,} samples ({samples/total_sample_size*100:.1f}%)")
        
        total_train_samples = 0
        total_test_samples = 0
        
        for category in categories:
            # This method is incomplete - will be fixed below
            pass
        
        return pd.DataFrame(), None
    
    def load_combined_data(self, categories: List[str], total_sample_size: Optional[int] = None) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:
        """Load and combine data from multiple categories using sequential loading"""
        print(f"\nðŸ“Š Loading combined dataset from {len(categories)} categories...")
        
        train_dfs = []
        
        # Calculate samples per category
        if total_sample_size:
            samples_per_category = {}
            for category in categories:
                if category in self.smart_all_sampling:
                    samples_per_category[category] = int(total_sample_size * self.smart_all_sampling[category])
                else:
                    # Equal distribution for unlisted categories
                    samples_per_category[category] = total_sample_size // len(categories)
            
            print(f"   Smart sampling distribution:")
            for cat, samples in samples_per_category.items():
                print(f"     {cat}: {samples:,} samples ({samples/total_sample_size*100:.1f}%)")
        
        total_train_samples = 0
        
        for category in categories:
            try:
                # Get target samples for this category
                if total_sample_size:
                    target_samples = samples_per_category.get(category, total_sample_size // len(categories))
                else:
                    target_samples = float('inf')  # Load all available
                
                # Find training files for this category
                train_pattern = self.data_dir / self.categories[category]
                train_files = glob.glob(str(train_pattern))
                
                if not train_files:
                    print(f"   âš ï¸  No training files found for {category} with pattern: {train_pattern}")
                    continue
                
                print(f"   Loading {category}: found {len(train_files)} file(s), target: {target_samples:,} samples")
                
                # Sequential loading - load files until we have enough samples
                category_dfs = []
                category_samples = 0
                files_loaded = 0
                
                for train_file in sorted(train_files):
                    if category_samples >= target_samples:
                        break
                        
                    try:
                        df = pd.read_parquet(train_file)
                        
                        # If this file would exceed our target, take only what we need
                        remaining_needed = target_samples - category_samples
                        if len(df) > remaining_needed:
                            df = df.head(int(remaining_needed))
                        
                        category_dfs.append(df)
                        category_samples += len(df)
                        files_loaded += 1
                        
                        # Break if we've reached our target
                        if category_samples >= target_samples:
                            break
                            
                    except Exception as e:
                        print(f"     âŒ Error loading {Path(train_file).name}: {e}")
                        continue
                
                if not category_dfs:
                    print(f"   âš ï¸  No data loaded for {category}")
                    continue
                
                # Combine files for this category
                category_df = pd.concat(category_dfs, ignore_index=True)
                
                # Add category identifier
                category_df['source_category'] = category
                train_dfs.append(category_df)
                total_train_samples += len(category_df)
                
                print(f"     âœ… {category}: loaded {len(category_df):,} samples from {files_loaded} files")
                
            except Exception as e:
                print(f"   âŒ Error loading {category}: {str(e)}")
                continue
        
        # Combine all dataframes
        combined_train_df = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()
        combined_test_df = None  # Skip test loading for combined training
        
        # Shuffle the combined dataset
        if not combined_train_df.empty:
            combined_train_df = combined_train_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"   ðŸ“Š Combined training data: {total_train_samples:,} samples from {len(train_dfs)} categories")
        print(f"   ðŸ“Š Test data loading skipped for combined training (memory optimization)")
        
        # Show label distribution
        if not combined_train_df.empty and 'label' in combined_train_df.columns:
            label_dist = combined_train_df['label'].value_counts().to_dict()
            print(f"   ðŸ“ˆ Combined label distribution: {label_dist}")
            
            # Show category distribution
            if 'source_category' in combined_train_df.columns:
                cat_dist = combined_train_df['source_category'].value_counts().to_dict()
                print(f"   ðŸ“Š Category distribution: {cat_dist}")
        
        return combined_train_df, combined_test_df
    
    def create_visualization_directory(self):
        """Create directory for saving visualization plots"""
        viz_dir = Path("visualizations")
        viz_dir.mkdir(exist_ok=True)
        return viz_dir
    
    def plot_roc_curve(self, y_true, y_pred_proba, model_type, category):
        """Plot ROC curve"""
        try:
            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
            roc_auc = auc(fpr, tpr)
            
            plt.figure(figsize=(8, 6))
            plt.plot(fpr, tpr, color='darkorange', lw=2, 
                    label=f'ROC curve (AUC = {roc_auc:.4f})')
            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.8)
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('False Positive Rate')
            plt.ylabel('True Positive Rate')
            plt.title(f'ROC Curve - {model_type.upper()} ({category})')
            plt.legend(loc="lower right")
            plt.grid(True, alpha=0.3)
            
            filename = self.viz_dir / f"roc_curve_{model_type}_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(filename)
        except Exception as e:
            print(f"   âš ï¸  Could not generate ROC curve: {e}")
            return None
    
    def plot_precision_recall_curve(self, y_true, y_pred_proba, model_type, category):
        """Plot Precision-Recall curve"""
        try:
            precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
            pr_auc = auc(recall, precision)
            
            plt.figure(figsize=(8, 6))
            plt.plot(recall, precision, color='blue', lw=2,
                    label=f'PR curve (AUC = {pr_auc:.4f})')
            plt.xlim([0.0, 1.0])
            plt.ylim([0.0, 1.05])
            plt.xlabel('Recall')
            plt.ylabel('Precision')
            plt.title(f'Precision-Recall Curve - {model_type.upper()} ({category})')
            plt.legend(loc="lower left")
            plt.grid(True, alpha=0.3)
            
            filename = self.viz_dir / f"pr_curve_{model_type}_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(filename)
        except Exception as e:
            print(f"   âš ï¸  Could not generate PR curve: {e}")
            return None
    
    def plot_confusion_matrix(self, y_true, y_pred, model_type, category):
        """Plot confusion matrix heatmap"""
        try:
            cm = confusion_matrix(y_true, y_pred)
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=['Benign', 'Malware'],
                       yticklabels=['Benign', 'Malware'])
            plt.title(f'Confusion Matrix - {model_type.upper()} ({category})')
            plt.ylabel('Actual')
            plt.xlabel('Predicted')
            
            filename = self.viz_dir / f"confusion_matrix_{model_type}_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(filename)
        except Exception as e:
            print(f"   âš ï¸  Could not generate confusion matrix: {e}")
            return None
    
    def plot_feature_importance(self, model, model_type, category, top_n=20):
        """Plot feature importance"""
        try:
            if model_type == 'lightgbm':
                importance = model.feature_importance(importance_type='gain')
                feature_names = model.feature_name()
            elif model_type == 'xgboost':
                importance = list(model.get_score(importance_type='gain').values())
                feature_names = list(model.get_score(importance_type='gain').keys())
            else:
                return None
            
            # Create feature importance dataframe
            feature_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False).head(top_n)
            
            plt.figure(figsize=(10, 8))
            sns.barplot(data=feature_df, y='feature', x='importance', palette='viridis')
            plt.title(f'Top {top_n} Feature Importance - {model_type.upper()} ({category})')
            plt.xlabel('Importance Score')
            plt.ylabel('Features')
            plt.tight_layout()
            
            filename = self.viz_dir / f"feature_importance_{model_type}_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(filename)
        except Exception as e:
            print(f"   âš ï¸  Could not generate feature importance: {e}")
            return None
    
    def plot_metrics_comparison(self):
        """Plot comparison of metrics across models and categories"""
        try:
            if not self.results:
                return None
                
            # Prepare data for visualization
            plot_data = []
            for category, results in self.results.items():
                for model_type in ['lightgbm', 'xgboost']:
                    if model_type in results:
                        metrics = results[model_type]['metrics']
                        test_metrics = results[model_type].get('test_metrics', metrics)
                        
                        plot_data.append({
                            'Category': category,
                            'Model': model_type.upper(),
                            'Validation AUC': metrics['roc_auc'],
                            'Test AUC': test_metrics['roc_auc'],
                            'Test Accuracy': test_metrics['accuracy'],
                            'Test F1': test_metrics['f1_score'],
                            'Test Precision': test_metrics['precision'],
                            'Test Recall': test_metrics['recall']
                        })
            
            if not plot_data:
                return None
                
            df = pd.DataFrame(plot_data)
            
            # Create subplots
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Model Performance Comparison Across Categories', fontsize=16, fontweight='bold')
            
            # AUC Comparison
            sns.barplot(data=df, x='Category', y='Test AUC', hue='Model', ax=axes[0,0])
            axes[0,0].set_title('Test AUC Comparison')
            axes[0,0].set_ylim(0.5, 1.0)
            axes[0,0].tick_params(axis='x', rotation=45)
            
            # Accuracy Comparison
            sns.barplot(data=df, x='Category', y='Test Accuracy', hue='Model', ax=axes[0,1])
            axes[0,1].set_title('Test Accuracy Comparison')
            axes[0,1].set_ylim(0.5, 1.0)
            axes[0,1].tick_params(axis='x', rotation=45)
            
            # F1 Score Comparison
            sns.barplot(data=df, x='Category', y='Test F1', hue='Model', ax=axes[1,0])
            axes[1,0].set_title('Test F1 Score Comparison')
            axes[1,0].set_ylim(0.5, 1.0)
            axes[1,0].tick_params(axis='x', rotation=45)
            
            # Precision vs Recall
            sns.scatterplot(data=df, x='Test Recall', y='Test Precision', 
                           hue='Model', style='Category', s=100, ax=axes[1,1])
            axes[1,1].set_title('Precision vs Recall')
            axes[1,1].set_xlim(0.5, 1.0)
            axes[1,1].set_ylim(0.5, 1.0)
            
            plt.tight_layout()
            filename = self.viz_dir / "model_performance_comparison.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(filename)
        except Exception as e:
            print(f"   âš ï¸  Could not generate metrics comparison: {e}")
            return None
    
    def plot_cross_validation_results(self, cv_results, model_type, category):
        """Plot cross-validation results"""
        try:
            if 'cv_scores' not in cv_results:
                return None
                
            cv_scores = cv_results['cv_scores']
            folds = range(1, len(cv_scores) + 1)
            
            plt.figure(figsize=(10, 6))
            plt.bar(folds, cv_scores, alpha=0.7, color='skyblue', edgecolor='navy')
            plt.axhline(y=cv_results['cv_mean'], color='red', linestyle='--', 
                       label=f"Mean: {cv_results['cv_mean']:.4f}")
            plt.axhline(y=cv_results['cv_mean'] + cv_results['cv_std'], 
                       color='orange', linestyle=':', alpha=0.7,
                       label=f"Â±1 STD: {cv_results['cv_std']:.4f}")
            plt.axhline(y=cv_results['cv_mean'] - cv_results['cv_std'], 
                       color='orange', linestyle=':', alpha=0.7)
            
            plt.title(f'Cross-Validation Results - {model_type.upper()} ({category})')
            plt.xlabel('Fold')
            plt.ylabel('AUC Score')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.ylim(0.5, 1.0)
            
            filename = self.viz_dir / f"cv_results_{model_type}_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(filename)
        except Exception as e:
            print(f"   âš ï¸  Could not generate CV results plot: {e}")
            return None
    
    def plot_training_validation_curves(self, model, X_train, y_train, X_val, y_val, model_type, category):
        """Plot training and validation curves during model training"""
        try:
            print(f"   ðŸ“Š Generating training/validation curves...")
            
            if model_type == 'randomforest':
                print(f"   âš ï¸  Training curves not available for Random Forest (no iterative training)")
                return None
            
            # For tree-based models, we'll simulate the training process
            train_aucs = []
            val_aucs = []
            iterations = []
            
            if model_type == 'lightgbm':
                # For LightGBM, use the model's best iteration information
                if hasattr(model, 'best_iteration') and model.best_iteration > 0:
                    max_iterations = min(model.best_iteration + 50, 1000)  # Go a bit beyond best iteration
                    step_size = max(1, max_iterations // 50)  # Sample ~50 points
                    
                    # Create evaluation datasets
                    train_data = lgb.Dataset(X_train, label=y_train)
                    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
                    
                    # Get model parameters
                    params = {
                        'objective': 'binary',
                        'metric': 'binary_logloss',
                        'boosting_type': 'gbdt',
                        'num_leaves': self.args.lgb_num_leaves,
                        'learning_rate': self.args.lgb_learning_rate,
                        'feature_fraction': self.args.lgb_feature_fraction,
                        'bagging_fraction': self.args.lgb_bagging_fraction,
                        'bagging_freq': self.args.lgb_bagging_freq,
                        'min_child_samples': self.args.lgb_min_child_samples,
                        'random_state': self.args.random_state,
                        'verbose': -1
                    }
                    
                    # Train models with different numbers of iterations
                    for num_iter in range(step_size, max_iterations + 1, step_size):
                        temp_model = lgb.train(params, train_data, num_boost_round=num_iter, verbose_eval=False)
                        
                        # Make predictions
                        train_pred = temp_model.predict(X_train, num_iteration=num_iter)
                        val_pred = temp_model.predict(X_val, num_iteration=num_iter)
                        
                        # Calculate AUC
                        train_auc = roc_auc_score(y_train, train_pred)
                        val_auc = roc_auc_score(y_val, val_pred)
                        
                        train_aucs.append(train_auc)
                        val_aucs.append(val_auc)
                        iterations.append(num_iter)
                
            elif model_type == 'xgboost':
                # For XGBoost, similar approach
                if hasattr(model, 'best_iteration') and model.best_iteration > 0:
                    max_iterations = min(model.best_iteration + 50, 1000)
                    step_size = max(1, max_iterations // 50)
                    
                    # Get model parameters
                    params = {
                        'objective': 'binary:logistic',
                        'eval_metric': 'logloss',
                        'max_depth': self.args.xgb_max_depth,
                        'learning_rate': self.args.xgb_learning_rate,
                        'subsample': self.args.xgb_subsample,
                        'colsample_bytree': self.args.xgb_colsample_bytree,
                        'min_child_weight': self.args.xgb_min_child_weight,
                        'reg_alpha': self.args.xgb_reg_alpha,
                        'reg_lambda': self.args.xgb_reg_lambda,
                        'random_state': self.args.random_state,
                        'verbosity': 0
                    }
                    
                    # Create DMatrix
                    dtrain = xgb.DMatrix(X_train, label=y_train)
                    dval = xgb.DMatrix(X_val, label=y_val)
                    
                    # Train models with different numbers of iterations
                    for num_iter in range(step_size, max_iterations + 1, step_size):
                        temp_model = xgb.train(params, dtrain, num_boost_round=num_iter, verbose_eval=False)
                        
                        # Make predictions
                        train_pred = temp_model.predict(dtrain, iteration_range=(0, num_iter))
                        val_pred = temp_model.predict(dval, iteration_range=(0, num_iter))
                        
                        # Calculate AUC
                        train_auc = roc_auc_score(y_train, train_pred)
                        val_auc = roc_auc_score(y_val, val_pred)
                        
                        train_aucs.append(train_auc)
                        val_aucs.append(val_auc)
                        iterations.append(num_iter)
            
            if not train_aucs:
                print(f"   âš ï¸  Could not generate training curves for {model_type}")
                return None
            
            # Create the plot
            plt.figure(figsize=(12, 8))
            plt.plot(iterations, train_aucs, 'o-', label='Training AUC', linewidth=2, markersize=6, alpha=0.8)
            plt.plot(iterations, val_aucs, 's-', label='Validation AUC', linewidth=2, markersize=6, alpha=0.8)
            
            # Mark the best iteration if available
            if hasattr(model, 'best_iteration') and model.best_iteration > 0:
                best_iter = model.best_iteration
                if best_iter in iterations:
                    best_idx = iterations.index(best_iter)
                    plt.axvline(x=best_iter, color='red', linestyle='--', alpha=0.7, label=f'Best Iteration ({best_iter})')
                    plt.scatter([best_iter], [val_aucs[best_idx]], color='red', s=100, zorder=5)
            
            plt.title(f'Training and Validation Curves - {model_type.upper()} ({category})')
            plt.xlabel('Number of Iterations/Estimators')
            plt.ylabel('AUC Score')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.ylim(0.5, 1.0)
            
            # Add overfitting detection
            if len(train_aucs) > 10:  # Only if we have enough points
                # Find where validation starts to plateau or decrease
                val_diff = [val_aucs[i] - val_aucs[i-1] for i in range(1, len(val_aucs))]
                train_diff = [train_aucs[i] - train_aucs[i-1] for i in range(1, len(train_aucs))]
                
                # Calculate overfitting gap
                gaps = [t - v for t, v in zip(train_aucs, val_aucs)]
                max_gap = max(gaps)
                max_gap_iter = iterations[gaps.index(max_gap)]
                
                plt.annotate(f'Max Gap: {max_gap:.3f}\n@ Iteration {max_gap_iter}', 
                            xy=(max_gap_iter, train_aucs[gaps.index(max_gap)]),
                            xytext=(20, 20), textcoords='offset points',
                            bbox=dict(boxstyle='round,pad=0.3', facecolor='orange', alpha=0.7),
                            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
            
            plt.tight_layout()
            filename = self.viz_dir / f"training_validation_curves_{model_type}_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"   ðŸ“ˆ Training curves saved: {filename}")
            return str(filename)
            
        except Exception as e:
            print(f"   âš ï¸  Could not generate training/validation curves: {e}")
            return None
    
    def plot_accuracy_comparison(self, model_results, category):
        """Plot accuracy comparison across different metrics"""
        try:
            print(f"   ðŸ“Š Generating accuracy comparison...")
            
            if not model_results:
                return None
            
            # Filter out non-model keys
            model_keys = [key for key in model_results.keys() 
                         if key in ['lightgbm', 'xgboost', 'randomforest'] and isinstance(model_results[key], dict)]
            
            if not model_keys:
                print(f"   âš ï¸  No valid model results found for accuracy comparison")
                return None
            
            metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
            
            # Prepare data
            metric_data = {metric: [] for metric in metrics_to_plot}
            
            for model_type in model_keys:
                model_data = model_results[model_type]
                
                # Try to get metrics from test_metrics first, then validation metrics
                metrics_source = model_data.get('test_metrics', model_data.get('metrics', {}))
                
                for metric in metrics_to_plot:
                    # Handle different metric naming conventions
                    metric_value = 0
                    if metric == 'f1_score' and 'f1' in metrics_source:
                        metric_value = metrics_source['f1']
                    elif metric in metrics_source:
                        metric_value = metrics_source[metric]
                    
                    metric_data[metric].append(metric_value)
            
            # Create subplot
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            axes = axes.flatten()
            
            colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum']
            
            for i, metric in enumerate(metrics_to_plot):
                ax = axes[i]
                bars = ax.bar(model_keys, metric_data[metric], color=colors[i % len(colors)], alpha=0.7)
                ax.set_title(f'{metric.upper()} Comparison')
                ax.set_ylabel('Score')
                ax.set_ylim(0, 1.0)
                ax.grid(True, alpha=0.3)
                
                # Add value labels on bars
                for bar, value in zip(bars, metric_data[metric]):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # Use the last subplot for training time comparison
            if len(axes) > len(metrics_to_plot):
                ax = axes[len(metrics_to_plot)]
                training_times = []
                for model_type in model_keys:
                    model_data = model_results[model_type]
                    metrics_source = model_data.get('test_metrics', model_data.get('metrics', {}))
                    if 'training_time' in metrics_source:
                        training_times.append(metrics_source['training_time'] / 60)  # Convert to minutes
                    else:
                        training_times.append(0)
                
                bars = ax.bar(model_keys, training_times, color='orange', alpha=0.7)
                ax.set_title('Training Time Comparison')
                ax.set_ylabel('Time (minutes)')
                ax.grid(True, alpha=0.3)
                
                # Add value labels
                for bar, value in zip(bars, training_times):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                           f'{value:.1f}m', ha='center', va='bottom', fontweight='bold')
            
            plt.suptitle(f'Model Performance Comparison - {category}', fontsize=16, fontweight='bold')
            plt.tight_layout()
            
            filename = self.viz_dir / f"accuracy_comparison_{category}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            plt.close()
            
            return str(filename)
        except Exception as e:
            print(f"   âš ï¸  Could not generate accuracy comparison: {e}")
            return None
    
    def generate_summary_visualizations(self):
        """Generate summary visualizations after all training is complete"""
        print(f"\nðŸ“Š Generating summary visualizations...")
        
        summary_files = []
        
        # Overall metrics comparison
        comparison_file = self.plot_metrics_comparison()
        if comparison_file:
            summary_files.append(comparison_file)
        
        if summary_files:
            print(f"   ðŸ“ˆ Generated {len(summary_files)} summary visualization(s)")
            print(f"   ðŸ“‚ Visualizations saved to: {self.viz_dir}")
        
        return summary_files
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """Prepare features and target variable"""
        # Identify feature columns (exclude non-feature columns)
        exclude_cols = ['label', 'family', 'family_confidence', 'source_category']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        
        X = df[feature_cols].copy()
        y = df['label'].copy()
        
        print(f"   Features: {len(feature_cols)} columns")
        print(f"   Target distribution: {dict(y.value_counts())}")
        
        return X, y
    
    def sample_data(self, X: pd.DataFrame, y: pd.Series, sample_size: Optional[int]) -> Tuple[pd.DataFrame, pd.Series]:
        """Sample data if sample_size is specified"""
        if sample_size and sample_size < len(X):
            print(f"   Sampling {sample_size:,} samples from {len(X):,} total samples...")
            
            # Stratified sampling to maintain label distribution
            from sklearn.model_selection import train_test_split
            X_sampled, _, y_sampled, _ = train_test_split(
                X, y, train_size=sample_size, random_state=self.args.random_state,
                stratify=y
            )
            print(f"   Sampled distribution: {dict(y_sampled.value_counts())}")
            return X_sampled, y_sampled
        
        return X, y
    
    def train_lightgbm(self, X_train: pd.DataFrame, y_train: pd.Series, 
                      X_val: pd.DataFrame, y_val: pd.Series, category: str) -> Dict:
        """Train LightGBM model"""
        print(f"\nðŸš€ Training LightGBM for {category}...")
        
        # LightGBM parameters
        lgb_params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': self.args.lgb_num_leaves,
            'learning_rate': self.args.lgb_learning_rate,
            'feature_fraction': self.args.lgb_feature_fraction,
            'bagging_fraction': self.args.lgb_bagging_fraction,
            'bagging_freq': self.args.lgb_bagging_freq,
            'min_child_samples': self.args.lgb_min_child_samples,
            'random_state': self.args.random_state,
            'n_jobs': self.args.n_jobs,
            'verbose': -1
        }
        
        # Create datasets
        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
        
        # Train model
        start_time = time.time()
        model = lgb.train(
            lgb_params,
            train_data,
            num_boost_round=self.args.lgb_n_estimators,
            valid_sets=[val_data],
            callbacks=[lgb.early_stopping(self.args.early_stopping_rounds, verbose=False)]
        )
        training_time = time.time() - start_time
        
        # Make predictions
        y_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # Calculate metrics
        metrics = self.calculate_metrics(y_val, y_pred, y_pred_proba)
        metrics['training_time'] = training_time
        metrics['best_iteration'] = model.best_iteration
        
        # Save model
        model_path = self.output_dir / f"lightgbm_{category.lower()}_model.txt"
        model.save_model(str(model_path))
        
        print(f"   Training completed in {training_time:.2f}s")
        print(f"   Best iteration: {model.best_iteration}")
        print(f"   Validation AUC: {metrics['roc_auc']:.4f}")
        print(f"   Model saved: {model_path}")
        
        # Generate visualizations
        viz_files = []
        if not self.args.no_viz:
            print(f"   ðŸ“Š Generating visualizations...")
            
            # ROC and PR curves
            roc_file = self.plot_roc_curve(y_val, y_pred_proba, 'lightgbm', category)
            if roc_file: viz_files.append(roc_file)
            
            pr_file = self.plot_precision_recall_curve(y_val, y_pred_proba, 'lightgbm', category)
            if pr_file: viz_files.append(pr_file)
            
            # Confusion matrix
            cm_file = self.plot_confusion_matrix(y_val, y_pred, 'lightgbm', category)
            if cm_file: viz_files.append(cm_file)
            
            # Feature importance
            fi_file = self.plot_feature_importance(model, 'lightgbm', category)
            if fi_file: viz_files.append(fi_file)
            
            # Training and validation curves
            curves_file = self.plot_training_validation_curves(model, X_train, y_train, X_val, y_val, 'lightgbm', category)
            if curves_file: viz_files.append(curves_file)
            
            print(f"   ðŸ“ˆ Generated {len(viz_files)} visualization(s)")
        else:
            print(f"   ðŸ“Š Visualization generation disabled")
        
        return {
            'model': model,
            'model_path': str(model_path),
            'metrics': metrics,
            'predictions': {'y_true': y_val, 'y_pred': y_pred, 'y_pred_proba': y_pred_proba},
            'visualizations': viz_files
        }
    
    def train_xgboost(self, X_train: pd.DataFrame, y_train: pd.Series,
                     X_val: pd.DataFrame, y_val: pd.Series, category: str) -> Dict:
        """Train XGBoost model"""
        print(f"\nðŸš€ Training XGBoost for {category}...")
        
        # XGBoost parameters
        xgb_params = {
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'max_depth': self.args.xgb_max_depth,
            'learning_rate': self.args.xgb_learning_rate,
            'subsample': self.args.xgb_subsample,
            'colsample_bytree': self.args.xgb_colsample_bytree,
            'min_child_weight': self.args.xgb_min_child_weight,
            'reg_alpha': self.args.xgb_reg_alpha,
            'reg_lambda': self.args.xgb_reg_lambda,
            'random_state': self.args.random_state,
            'n_jobs': self.args.n_jobs
        }
        
        # Create DMatrix
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)
        
        # Train model
        start_time = time.time()
        model = xgb.train(
            xgb_params,
            dtrain,
            num_boost_round=self.args.xgb_n_estimators,
            evals=[(dval, 'validation')],
            early_stopping_rounds=self.args.early_stopping_rounds,
            verbose_eval=False
        )
        training_time = time.time() - start_time
        
        # Make predictions
        y_pred_proba = model.predict(dval, iteration_range=(0, model.best_iteration))
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # Calculate metrics
        metrics = self.calculate_metrics(y_val, y_pred, y_pred_proba)
        metrics['training_time'] = training_time
        metrics['best_iteration'] = model.best_iteration
        
        # Save model
        model_path = self.output_dir / f"xgboost_{category.lower()}_model.json"
        model.save_model(str(model_path))
        
        print(f"   Training completed in {training_time:.2f}s")
        print(f"   Best iteration: {model.best_iteration}")
        print(f"   Validation AUC: {metrics['roc_auc']:.4f}")
        print(f"   Model saved: {model_path}")
        
        # Generate visualizations
        viz_files = []
        if not self.args.no_viz:
            print(f"   ðŸ“Š Generating visualizations...")
            
            # ROC and PR curves
            roc_file = self.plot_roc_curve(y_val, y_pred_proba, 'xgboost', category)
            if roc_file: viz_files.append(roc_file)
            
            pr_file = self.plot_precision_recall_curve(y_val, y_pred_proba, 'xgboost', category)
            if pr_file: viz_files.append(pr_file)
            
            # Confusion matrix
            cm_file = self.plot_confusion_matrix(y_val, y_pred, 'xgboost', category)
            if cm_file: viz_files.append(cm_file)
            
            # Feature importance
            fi_file = self.plot_feature_importance(model, 'xgboost', category)
            if fi_file: viz_files.append(fi_file)
            
            # Training and validation curves
            curves_file = self.plot_training_validation_curves(model, X_train, y_train, X_val, y_val, 'xgboost', category)
            if curves_file: viz_files.append(curves_file)
            
            print(f"   ðŸ“ˆ Generated {len(viz_files)} visualization(s)")
        else:
            print(f"   ðŸ“Š Visualization generation disabled")
        
        return {
            'model': model,
            'model_path': str(model_path),
            'metrics': metrics,
            'predictions': {'y_true': y_val, 'y_pred': y_pred, 'y_pred_proba': y_pred_proba},
            'visualizations': viz_files
        }
    
    def train_randomforest(self, X_train: pd.DataFrame, y_train: pd.Series,
                          X_val: pd.DataFrame, y_val: pd.Series, category: str) -> Dict:
        """Train Random Forest model"""
        print(f"\nðŸš€ Training Random Forest for {category}...")
        
        # Random Forest parameters
        rf_params = {
            'n_estimators': getattr(self.args, 'rf_n_estimators', 200),
            'max_depth': getattr(self.args, 'rf_max_depth', 10),
            'min_samples_split': getattr(self.args, 'rf_min_samples_split', 10),
            'min_samples_leaf': getattr(self.args, 'rf_min_samples_leaf', 5),
            'max_features': getattr(self.args, 'rf_max_features', 'sqrt'),
            'bootstrap': getattr(self.args, 'rf_bootstrap', True),
            'oob_score': getattr(self.args, 'rf_oob_score', True),
            'class_weight': getattr(self.args, 'rf_class_weight', None),
            'random_state': self.args.random_state,
            'n_jobs': self.args.n_jobs,
            'verbose': 0
        }
        
        # Remove None values
        rf_params = {k: v for k, v in rf_params.items() if v is not None}
        
        print(f"   Random Forest parameters: {rf_params}")
        
        # Create and train model
        model = RandomForestClassifier(**rf_params)
        
        start_time = time.time()
        # Handle NaN values for Random Forest (it doesn't handle them natively)
        X_train_clean = X_train.fillna(X_train.median())
        X_val_clean = X_val.fillna(X_train.median())  # Use training median for consistency
        
        model.fit(X_train_clean, y_train)
        training_time = time.time() - start_time
        
        # Make predictions
        y_pred_proba = model.predict_proba(X_val_clean)[:, 1]
        y_pred = model.predict(X_val_clean)
        
        # Calculate metrics
        metrics = self.calculate_metrics(y_val, y_pred, y_pred_proba)
        metrics['training_time'] = training_time
        metrics['n_estimators'] = model.n_estimators
        
        # OOB Score if available
        if getattr(self.args, 'rf_oob_score', True) and hasattr(model, 'oob_score_'):
            metrics['oob_score'] = model.oob_score_
            print(f"   OOB Score: {model.oob_score_:.4f}")
        
        # Save model
        model_path = self.output_dir / f"randomforest_{category.lower()}_model.joblib"
        joblib.dump(model, str(model_path))
        
        print(f"   Training completed in {training_time:.2f}s")
        print(f"   Number of estimators: {model.n_estimators}")
        print(f"   Validation AUC: {metrics['roc_auc']:.4f}")
        print(f"   Model saved: {model_path}")
        
        # Generate visualizations
        viz_files = []
        if not self.args.no_viz:
            print(f"   ðŸ“Š Generating visualizations...")
            
            # ROC and PR curves
            roc_file = self.plot_roc_curve(y_val, y_pred_proba, 'randomforest', category)
            if roc_file: viz_files.append(roc_file)
            
            pr_file = self.plot_precision_recall_curve(y_val, y_pred_proba, 'randomforest', category)
            if pr_file: viz_files.append(pr_file)
            
            # Confusion matrix
            cm_file = self.plot_confusion_matrix(y_val, y_pred, 'randomforest', category)
            if cm_file: viz_files.append(cm_file)
            
            # Feature importance
            fi_file = self.plot_feature_importance(model, 'randomforest', category)
            if fi_file: viz_files.append(fi_file)
            
            print(f"   ðŸ“ˆ Generated {len(viz_files)} visualization(s)")
        else:
            print(f"   ðŸ“Š Visualization generation disabled")
        
        return {
            'model': model,
            'model_path': str(model_path),
            'metrics': metrics,
            'predictions': {'y_true': y_val, 'y_pred': y_pred, 'y_pred_proba': y_pred_proba},
            'visualizations': viz_files
        }
    
    def calculate_metrics(self, y_true: pd.Series, y_pred: np.ndarray, y_pred_proba: np.ndarray) -> Dict:
        """Calculate comprehensive evaluation metrics"""
        # Handle binary classification probabilities
        if len(y_pred_proba.shape) > 1 and y_pred_proba.shape[1] > 1:
            y_pred_proba = y_pred_proba[:, 1]
        elif len(y_pred_proba.shape) > 1:
            y_pred_proba = y_pred_proba.flatten()
        
        # Ensure we have proper arrays
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        y_pred_proba = np.array(y_pred_proba)
        
        # Basic classification metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='binary', zero_division=0)
        recall = recall_score(y_true, y_pred, average='binary', zero_division=0)
        f1 = f1_score(y_true, y_pred, average='binary', zero_division=0)
        
        # ROC metrics
        roc_auc = roc_auc_score(y_true, y_pred_proba)
        
        # Precision-Recall metrics
        try:
            precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred_proba)
            pr_auc = auc(recall_curve, precision_curve)
            avg_precision = average_precision_score(y_true, y_pred_proba)
        except:
            pr_auc = 0.0
            avg_precision = 0.0
        
        # Advanced metrics
        mcc = matthews_corrcoef(y_true, y_pred)
        kappa = cohen_kappa_score(y_true, y_pred)
        balanced_acc = balanced_accuracy_score(y_true, y_pred)
        
        # Probability metrics
        try:
            log_loss_score = log_loss(y_true, y_pred_proba)
            brier_score = brier_score_loss(y_true, y_pred_proba)
        except:
            log_loss_score = float('inf')
            brier_score = float('inf')
        
        # Confusion matrix elements
        try:
            cm = confusion_matrix(y_true, y_pred)
            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
            else:
                # Handle edge case where only one class is present
                if len(np.unique(y_true)) == 1:
                    if y_true[0] == 0:
                        tn, fp, fn, tp = len(y_true), 0, 0, 0
                    else:
                        tn, fp, fn, tp = 0, 0, 0, len(y_true)
                else:
                    tn, fp, fn, tp = 0, 0, 0, 0
        except:
            tn, fp, fn, tp = 0, 0, 0, 0
        
        # Derived metrics
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        sensitivity = recall  # Same as recall/true positive rate
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0  # False positive rate
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0  # False negative rate
        
        # Positive and negative predictive values
        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Same as precision
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0  # Negative predictive value
        
        return {
            # Basic metrics
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'specificity': float(specificity),
            'sensitivity': float(sensitivity),
            'balanced_accuracy': float(balanced_acc),
            
            # ROC and PR metrics
            'roc_auc': float(roc_auc),
            'pr_auc': float(pr_auc),
            'avg_precision': float(avg_precision),
            
            # Advanced metrics
            'matthews_corrcoef': float(mcc),
            'cohen_kappa': float(kappa),
            'log_loss': float(log_loss_score),
            'brier_score': float(brier_score),
            
            # Rates and predictive values
            'false_positive_rate': float(fpr),
            'false_negative_rate': float(fnr),
            'positive_predictive_value': float(ppv),
            'negative_predictive_value': float(npv),
            
            # Confusion matrix elements
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
        }
    
    def evaluate_on_test_set(self, model_info: Dict, X_test: pd.DataFrame, y_test: pd.Series, 
                           model_type: str, category: str) -> Dict:
        """Evaluate model on test set"""
        print(f"\nðŸ” Evaluating {model_type} on {category} test set...")
        
        model = model_info['model']
        
        # Make predictions based on model type
        if model_type == 'lightgbm':
            y_pred_proba = model.predict(X_test, num_iteration=model.best_iteration)
        elif model_type == 'xgboost':
            dtest = xgb.DMatrix(X_test)
            y_pred_proba = model.predict(dtest, iteration_range=(0, model.best_iteration))
        else:  # randomforest
            # Handle NaN values for Random Forest
            X_test_clean = X_test.fillna(X_test.median())
            y_pred_proba = model.predict_proba(X_test_clean)[:, 1]
        
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # Calculate metrics
        test_metrics = self.calculate_metrics(y_test, y_pred, y_pred_proba)
        
        print(f"   Test AUC: {test_metrics['roc_auc']:.4f}")
        print(f"   Test Accuracy: {test_metrics['accuracy']:.4f}")
        print(f"   Test F1-Score: {test_metrics['f1_score']:.4f}")
        print(f"   Test Precision: {test_metrics['precision']:.4f}")
        print(f"   Test Recall: {test_metrics['recall']:.4f}")
        print(f"   Test Specificity: {test_metrics['specificity']:.4f}")
        print(f"   Test MCC: {test_metrics['matthews_corrcoef']:.4f}")
        
        return test_metrics
    
    def cross_validate_model(self, X: pd.DataFrame, y: pd.Series, model_type: str, category: str) -> Dict:
        """Perform cross-validation"""
        print(f"\nðŸ”„ Cross-validating {model_type} for {category}...")
        
        cv = StratifiedKFold(n_splits=self.args.cv_folds, shuffle=True, random_state=self.args.random_state)
        
        if model_type == 'lightgbm':
            model = lgb.LGBMClassifier(
                num_leaves=self.args.lgb_num_leaves,
                learning_rate=self.args.lgb_learning_rate,
                feature_fraction=self.args.lgb_feature_fraction,
                bagging_fraction=self.args.lgb_bagging_fraction,
                bagging_freq=self.args.lgb_bagging_freq,
                min_child_samples=self.args.lgb_min_child_samples,
                n_estimators=self.args.lgb_n_estimators,
                random_state=self.args.random_state,
                n_jobs=self.args.n_jobs,
                verbose=-1
            )
        elif model_type == 'xgboost':
            model = xgb.XGBClassifier(
                max_depth=self.args.xgb_max_depth,
                learning_rate=self.args.xgb_learning_rate,
                subsample=self.args.xgb_subsample,
                colsample_bytree=self.args.xgb_colsample_bytree,
                min_child_weight=self.args.xgb_min_child_weight,
                reg_alpha=self.args.xgb_reg_alpha,
                reg_lambda=self.args.xgb_reg_lambda,
                n_estimators=self.args.xgb_n_estimators,
                random_state=self.args.random_state,
                n_jobs=self.args.n_jobs,
                eval_metric='logloss',
                verbosity=0
            )
        else:  # randomforest
            model = RandomForestClassifier(
                n_estimators=getattr(self.args, 'rf_n_estimators', 200),
                max_depth=getattr(self.args, 'rf_max_depth', 10),
                min_samples_split=getattr(self.args, 'rf_min_samples_split', 10),
                min_samples_leaf=getattr(self.args, 'rf_min_samples_leaf', 5),
                max_features=getattr(self.args, 'rf_max_features', 'sqrt'),
                bootstrap=getattr(self.args, 'rf_bootstrap', True),
                oob_score=getattr(self.args, 'rf_oob_score', True),
                class_weight=getattr(self.args, 'rf_class_weight', None),
                random_state=self.args.random_state,
                n_jobs=self.args.n_jobs,
                verbose=0
            )
        
        # Handle NaN values for Random Forest
        if model_type == 'randomforest':
            X = X.fillna(X.median())
        
        # Perform cross-validation
        cv_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc', n_jobs=1)
        
        cv_results = {
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'cv_scores': cv_scores.tolist()
        }
        
        print(f"   CV AUC: {cv_results['cv_mean']:.4f} Â± {cv_results['cv_std']:.4f}")
        
        return cv_results
    
    def train_category(self, categories: Union[str, List[str]]) -> Dict:
        """Train models for specific category(ies)"""
        if isinstance(categories, str):
            categories = [categories]
            category_name = categories[0]
        else:
            category_name = "combined_" + "_".join(categories)
        
        print(f"\n{'='*60}")
        print(f"ðŸŽ¯ TRAINING MODELS FOR {category_name.upper()}")
        print(f"{'='*60}")
        
        # Load data - single or combined
        if len(categories) == 1:
            train_df, test_df = self.load_data(categories[0])
        else:
            train_df, test_df = self.load_combined_data(categories, self.args.sample_size)
        
        if train_df is None or train_df.empty:
            raise ValueError(f"No training data loaded for categories: {categories}")
            
        # Prepare features
        X_train_full, y_train_full = self.prepare_features(train_df)
        
        # Additional sampling if requested and not already done in load_data
        if self.args.sample_size and len(categories) == 1 and self.args.sample_size < len(X_train_full):
            X_train_full, y_train_full = self.sample_data(X_train_full, y_train_full, self.args.sample_size)
        
        # Determine validation strategy - remove automatic challenge set for all categories
        use_challenge_validation = self.args.use_challenge_validation  # Only if explicitly requested
        
        if use_challenge_validation:
            # Use challenge set for validation when training on all categories or explicitly requested
            challenge_df = self.load_challenge_set()
            if challenge_df is not None and not challenge_df.empty:
                # Check if challenge set has balanced classes
                challenge_labels = challenge_df['label'].value_counts()
                has_balanced_classes = len(challenge_labels) > 1 and min(challenge_labels) > 100
                
                if has_balanced_classes:
                    # Use challenge set directly if it has balanced classes
                    X_val, y_val = self.prepare_features(challenge_df)
                    X_train, y_train = X_train_full, y_train_full  # Use full training data
                    validation_approach = "challenge_set_direct"
                else:
                    # Create balanced validation set using challenge + benign from training
                    X_train, X_val, y_train, y_val = self.create_balanced_challenge_validation(train_df, challenge_df)
                    validation_approach = "challenge_set_balanced"
                
                if len(categories) > 1:
                    print(f"   ðŸŽ¯ Using challenge set for validation - combined training mode ({len(y_val):,} samples)")
                else:
                    print(f"   ðŸŽ¯ Using challenge set for validation - user requested ({len(y_val):,} samples)")
                print(f"   ðŸ“Š Training on {len(y_train):,} samples")
                
                # Store validation type in results for reporting
                validation_type = validation_approach
            else:
                # Fallback to standard train/val split if challenge set not available
                print(f"   âš ï¸  Challenge set not available, using standard train/val split")
                use_challenge_validation = False
        
        if not use_challenge_validation:
            # Standard train/validation split for single category or fallback
            X_train, X_val, y_train, y_val = train_test_split(
                X_train_full, y_train_full, 
                test_size=self.args.val_size, 
                random_state=self.args.random_state,
                stratify=y_train_full
            )
            print(f"   ðŸ“Š Using standard train/validation split")
            validation_type = "train_val_split"
        
        print(f"   Train set: {X_train.shape[0]:,} samples")
        print(f"   Validation set: {X_val.shape[0]:,} samples")
        
        category_results = {
            'category': category_name,
            'validation_type': validation_type,
            'train_samples': len(X_train),
            'validation_samples': len(X_val)
        }
        
        # Train requested models
        if 'lightgbm' in self.args.models:
            lgb_results = self.train_lightgbm(X_train, y_train, X_val, y_val, category_name)
            category_results['lightgbm'] = lgb_results
            
            # Cross-validation
            if self.args.cv_folds > 1:
                lgb_cv = self.cross_validate_model(X_train_full, y_train_full, 'lightgbm', category_name)
                category_results['lightgbm']['cv_results'] = lgb_cv
                
                # Plot CV results
                if not self.args.no_viz:
                    cv_file = self.plot_cross_validation_results(lgb_cv, 'lightgbm', category_name)
                    if cv_file:
                        category_results['lightgbm']['visualizations'].append(cv_file)
            
            # Test set evaluation (skip for combined training to save memory)
            if test_df is not None and len(categories) == 1:
                X_test, y_test = self.prepare_features(test_df)
                test_metrics = self.evaluate_on_test_set(lgb_results, X_test, y_test, 'lightgbm', category_name)
                category_results['lightgbm']['test_metrics'] = test_metrics
            elif len(categories) > 1:
                print(f"   ðŸ“Š Test set evaluation skipped for combined training (memory optimization)")
        
        if 'xgboost' in self.args.models:
            xgb_results = self.train_xgboost(X_train, y_train, X_val, y_val, category_name)
            category_results['xgboost'] = xgb_results
            
            # Cross-validation
            if self.args.cv_folds > 1:
                xgb_cv = self.cross_validate_model(X_train_full, y_train_full, 'xgboost', category_name)
                category_results['xgboost']['cv_results'] = xgb_cv
                
                # Plot CV results
                if not self.args.no_viz:
                    cv_file = self.plot_cross_validation_results(xgb_cv, 'xgboost', category_name)
                    if cv_file:
                        category_results['xgboost']['visualizations'].append(cv_file)
            
            # Test set evaluation (skip for combined training to save memory)
            if test_df is not None and len(categories) == 1:
                X_test, y_test = self.prepare_features(test_df)
                test_metrics = self.evaluate_on_test_set(xgb_results, X_test, y_test, 'xgboost', category_name)
                category_results['xgboost']['test_metrics'] = test_metrics
            elif len(categories) > 1:
                print(f"   ðŸ“Š Test set evaluation skipped for combined training (memory optimization)")
        
        if 'randomforest' in self.args.models:
            rf_results = self.train_randomforest(X_train, y_train, X_val, y_val, category_name)
            category_results['randomforest'] = rf_results
            
            # Cross-validation
            if self.args.cv_folds > 1:
                rf_cv = self.cross_validate_model(X_train_full, y_train_full, 'randomforest', category_name)
                category_results['randomforest']['cv_results'] = rf_cv
                
                # Plot CV results
                if not self.args.no_viz:
                    cv_file = self.plot_cross_validation_results(rf_cv, 'randomforest', category_name)
                    if cv_file:
                        category_results['randomforest']['visualizations'].append(cv_file)
            
            # Test set evaluation (skip for combined training to save memory)
            if test_df is not None and len(categories) == 1:
                X_test, y_test = self.prepare_features(test_df)
                test_metrics = self.evaluate_on_test_set(rf_results, X_test, y_test, 'randomforest', category_name)
                category_results['randomforest']['test_metrics'] = test_metrics
            elif len(categories) > 1:
                print(f"   ðŸ“Š Test set evaluation skipped for combined training (memory optimization)")
        
        # Generate accuracy comparison visualization
        model_keys = [key for key in category_results.keys() 
                     if key in ['lightgbm', 'xgboost', 'randomforest'] and isinstance(category_results[key], dict)]
        
        if not self.args.no_viz and len(model_keys) > 1:
            accuracy_file = self.plot_accuracy_comparison(category_results, category_name)
            if accuracy_file:
                for model_type in model_keys:
                    if 'visualizations' in category_results[model_type]:
                        category_results[model_type]['visualizations'].append(accuracy_file)
        
        return category_results
    
    def save_results(self):
        """Save training results to JSON file"""
        # Convert non-serializable objects to serializable format
        serializable_results = {}
        for category, results in self.results.items():
            serializable_results[category] = {}
            serializable_results[category]['category'] = results['category']
            
            for model_type in ['lightgbm', 'xgboost', 'randomforest']:
                if model_type in results:
                    model_results = results[model_type].copy()
                    # Remove non-serializable objects
                    model_results.pop('model', None)
                    model_results.pop('predictions', None)
                    serializable_results[category][model_type] = model_results
        
        # Save results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.output_dir / f"training_results_{timestamp}.json"
        
        with open(results_file, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"\nðŸ’¾ Results saved to: {results_file}")
    
    def run_training(self):
        """Main training pipeline"""
        print(f"\nðŸš€ EMBER2024 Malware Detection Model Training")
        print(f"{'='*60}")
        print(f"Models: {', '.join(self.args.models)}")
        print(f"Categories: {', '.join(self.args.category)}")
        print(f"Sample size: {self.args.sample_size or 'Full dataset'}")
        print(f"Output directory: {self.output_dir}")
        
        # Show validation strategy
        if len(self.args.category) > 1 or self.args.use_challenge_validation:
            print(f"ðŸŽ¯ Validation strategy: Challenge set (malicious samples)")
        else:
            print(f"ðŸ“Š Validation strategy: Standard train/validation split ({self.args.val_size*100:.0f}%)")
        
        start_time = time.time()
        
        # Handle "all" category specially
        if len(self.args.category) > 1 or (len(self.args.category) == 1 and 'all' in self.args.category[0]):
            # Combined training for multiple categories
            valid_categories = []
            for category in self.args.category:
                if category in self.categories:
                    valid_categories.append(category)
                else:
                    print(f"âŒ Invalid category: {category}")
            
            if valid_categories:
                try:
                    category_key = "combined_" + "_".join(valid_categories)
                    self.results[category_key] = self.train_category(valid_categories)
                except Exception as e:
                    print(f"âŒ Error training combined categories {valid_categories}: {str(e)}")
        else:
            # Single category training
            for category in self.args.category:
                if category not in self.categories:
                    print(f"âŒ Invalid category: {category}")
                    continue
                
                try:
                    self.results[category] = self.train_category(category)
                except Exception as e:
                    print(f"âŒ Error training {category}: {str(e)}")
                    continue
        
        total_time = time.time() - start_time
        
        # Save results
        self.save_results()
        
        # Generate summary visualizations
        if not self.args.no_viz:
            summary_viz = self.generate_summary_visualizations()
        else:
            print(f"\nðŸ“Š Summary visualization generation disabled")
        
        # Print summary
        self.print_summary(total_time)
    
    def print_summary(self, total_time: float):
        """Print training summary"""
        print(f"\n{'='*60}")
        print(f"ðŸŽ‰ TRAINING COMPLETE!")
        print(f"{'='*60}")
        print(f"Total training time: {total_time:.2f}s ({total_time/60:.2f} minutes)")
        print(f"Categories trained: {len(self.results)}")
        
        print(f"\nðŸ“Š RESULTS SUMMARY:")
        print(f"{'Category':<20} {'Model':<10} {'Validation':<12} {'Val AUC':<8} {'Test AUC':<8} {'CV AUC':<8} {'Accuracy':<8} {'Precision':<9} {'Recall':<8} {'F1':<8} {'MCC':<8}")
        print(f"{'-'*115}")
        
        for category, results in self.results.items():
            validation_type = results.get('validation_type', 'unknown')
            val_type_short = 'Challenge' if 'challenge' in validation_type else 'TrainVal'
            
            for model_type in ['lightgbm', 'xgboost', 'randomforest']:
                if model_type in results:
                    val_metrics = results[model_type]['metrics']
                    test_metrics = results[model_type].get('test_metrics', {})
                    cv_results = results[model_type].get('cv_results', {})
                    
                    val_auc = val_metrics['roc_auc']
                    test_auc = test_metrics.get('roc_auc', 'N/A')
                    cv_auc = cv_results.get('cv_mean', 'N/A')
                    accuracy = test_metrics.get('accuracy', val_metrics.get('accuracy', 'N/A'))
                    precision = test_metrics.get('precision', val_metrics.get('precision', 'N/A'))
                    recall = test_metrics.get('recall', val_metrics.get('recall', 'N/A'))
                    f1 = test_metrics.get('f1_score', val_metrics.get('f1_score', 'N/A'))
                    mcc = test_metrics.get('matthews_corrcoef', val_metrics.get('matthews_corrcoef', 'N/A'))
                    
                    # Format values
                    test_auc_str = f"{test_auc:.4f}" if test_auc != 'N/A' else 'N/A'
                    cv_auc_str = f"{cv_auc:.4f}" if cv_auc != 'N/A' else 'N/A'
                    accuracy_str = f"{accuracy:.4f}" if accuracy != 'N/A' else 'N/A'
                    precision_str = f"{precision:.4f}" if precision != 'N/A' else 'N/A'
                    recall_str = f"{recall:.4f}" if recall != 'N/A' else 'N/A'
                    f1_str = f"{f1:.4f}" if f1 != 'N/A' else 'N/A'
                    mcc_str = f"{mcc:.4f}" if mcc != 'N/A' else 'N/A'
                    
                    print(f"{category:<20} {model_type:<10} {val_type_short:<12} {val_auc:<8.4f} {test_auc_str:<8} {cv_auc_str:<8} {accuracy_str:<8} {precision_str:<9} {recall_str:<8} {f1_str:<8} {mcc_str:<8}")
        
        print(f"\nðŸ“ˆ DETAILED METRICS BREAKDOWN:")
        for category, results in self.results.items():
            validation_type = results.get('validation_type', 'unknown')
            train_samples = results.get('train_samples', 'N/A')
            val_samples = results.get('validation_samples', 'N/A')
            
            print(f"\nðŸŽ¯ {category.upper()}:")
            print(f"   ðŸ“Š Validation Strategy: {validation_type}")
            print(f"   ðŸ“ˆ Training Samples: {train_samples:,} | Validation Samples: {val_samples:,}")
            
            for model_type in ['lightgbm', 'xgboost', 'randomforest']:
                if model_type in results:
                    print(f"  ðŸ“Š {model_type.upper()}:")
                    
                    # Validation metrics
                    val_metrics = results[model_type]['metrics']
                    if 'challenge' in validation_type:
                        validation_label = "Challenge Set Validation"
                        if validation_type == 'challenge_set_balanced':
                            validation_label += " (Balanced)"
                        print(f"     {validation_label} - AUC: {val_metrics['roc_auc']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}")
                        print(f"     {validation_label} - Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}")
                        print(f"     {validation_label} - F1: {val_metrics['f1_score']:.4f}, MCC: {val_metrics['matthews_corrcoef']:.4f}")
                    else:
                        print(f"     Validation - AUC: {val_metrics['roc_auc']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}")
                        print(f"     Validation - Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}")
                        print(f"     Validation - F1: {val_metrics['f1_score']:.4f}, MCC: {val_metrics['matthews_corrcoef']:.4f}")
                    
                    # OOB Score for Random Forest
                    if model_type == 'randomforest' and 'oob_score' in val_metrics:
                        print(f"     Out-of-Bag Score: {val_metrics['oob_score']:.4f}")
                    
                    # Test metrics if available
                    if 'test_metrics' in results[model_type]:
                        test_metrics = results[model_type]['test_metrics']
                        print(f"     Test - AUC: {test_metrics['roc_auc']:.4f}, Accuracy: {test_metrics['accuracy']:.4f}")
                        print(f"     Test - Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}")
                        print(f"     Test - Specificity: {test_metrics['specificity']:.4f}, NPV: {test_metrics['negative_predictive_value']:.4f}")
                        print(f"     Test - Confusion Matrix: TP={test_metrics['true_positives']}, TN={test_metrics['true_negatives']}, FP={test_metrics['false_positives']}, FN={test_metrics['false_negatives']}")
                    
                    # CV metrics if available
                    if 'cv_results' in results[model_type]:
                        cv_results = results[model_type]['cv_results']
                        print(f"     Cross-validation - AUC: {cv_results['cv_mean']:.4f} Â± {cv_results['cv_std']:.4f}")
                    
                    print()


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="EMBER2024 Malware Detection Model Training Script",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Dataset arguments
    parser.add_argument('--data_dir', type=str, default='ember2024_ultra_focused_features',
                       help='Directory containing split parquet files')
    parser.add_argument('--category', nargs='+', 
                       choices=['Win32', 'Win64', 'APK', 'PDF', 'ELF', 'Net', 'all'],
                       default=['Win32'], help='Dataset categories to train on')
    parser.add_argument('--sample_size', type=int, default=None,
                       help='Number of samples to use (None for full dataset)')
    
    # Model selection
    parser.add_argument('--models', nargs='+', choices=['lightgbm', 'xgboost', 'randomforest'], 
                       default=['lightgbm'], help='Models to train')
    
    # Training arguments
    parser.add_argument('--val_size', type=float, default=0.2,
                       help='Validation set size (fraction of training data)')
    parser.add_argument('--use_challenge_validation', action='store_true',
                       help='Use challenge set for validation (automatically enabled for all categories)')
    parser.add_argument('--challenge_balance_ratio', type=float, default=1.0,
                       help='Ratio of benign to malicious samples in challenge validation (default: 1.0 for balanced)')
    parser.add_argument('--cv_folds', type=int, default=0,
                       help='Number of cross-validation folds (0 to skip CV)')
    parser.add_argument('--random_state', type=int, default=42,
                       help='Random state for reproducibility')
    parser.add_argument('--n_jobs', type=int, default=-1,
                       help='Number of parallel jobs')
    parser.add_argument('--early_stopping_rounds', type=int, default=100,
                       help='Early stopping rounds')
    
    # LightGBM parameters
    parser.add_argument('--lgb_n_estimators', type=int, default=1000,
                       help='LightGBM number of estimators')
    parser.add_argument('--lgb_num_leaves', type=int, default=31,
                       help='LightGBM number of leaves')
    parser.add_argument('--lgb_learning_rate', type=float, default=0.1,
                       help='LightGBM learning rate')
    parser.add_argument('--lgb_feature_fraction', type=float, default=0.9,
                       help='LightGBM feature fraction')
    parser.add_argument('--lgb_bagging_fraction', type=float, default=0.8,
                       help='LightGBM bagging fraction')
    parser.add_argument('--lgb_bagging_freq', type=int, default=5,
                       help='LightGBM bagging frequency')
    parser.add_argument('--lgb_min_child_samples', type=int, default=20,
                       help='LightGBM minimum child samples')
    
    # XGBoost parameters
    parser.add_argument('--xgb_n_estimators', type=int, default=1000,
                       help='XGBoost number of estimators')
    parser.add_argument('--xgb_max_depth', type=int, default=6,
                       help='XGBoost maximum depth')
    parser.add_argument('--xgb_learning_rate', type=float, default=0.1,
                       help='XGBoost learning rate')
    parser.add_argument('--xgb_subsample', type=float, default=0.8,
                       help='XGBoost subsample')
    parser.add_argument('--xgb_colsample_bytree', type=float, default=0.8,
                       help='XGBoost column sample by tree')
    parser.add_argument('--xgb_min_child_weight', type=int, default=1,
                       help='XGBoost minimum child weight')
    parser.add_argument('--xgb_reg_alpha', type=float, default=0,
                       help='XGBoost L1 regularization')
    parser.add_argument('--xgb_reg_lambda', type=float, default=1,
                       help='XGBoost L2 regularization')
    
    # Random Forest parameters
    parser.add_argument('--rf_n_estimators', type=int, default=200,
                       help='Random Forest number of estimators (trees)')
    parser.add_argument('--rf_max_depth', type=int, default=10,
                       help='Random Forest maximum depth (None for unlimited)')
    parser.add_argument('--rf_min_samples_split', type=int, default=10,
                       help='Random Forest minimum samples to split internal node')
    parser.add_argument('--rf_min_samples_leaf', type=int, default=5,
                       help='Random Forest minimum samples in leaf node')
    parser.add_argument('--rf_max_features', type=str, default='sqrt',
                       choices=['sqrt', 'log2', 'auto', 'None'],
                       help='Random Forest max features to consider for best split')
    parser.add_argument('--rf_bootstrap', type=bool, default=True,
                       help='Random Forest whether to use bootstrap samples')
    parser.add_argument('--rf_oob_score', type=bool, default=True,
                       help='Random Forest compute out-of-bag score')
    parser.add_argument('--rf_class_weight', type=str, default=None,
                       choices=['balanced', 'balanced_subsample', 'None'],
                       help='Random Forest class weights')
    
    # Output arguments
    parser.add_argument('--output_dir', type=str, default='models',
                       help='Output directory for models and results')
    parser.add_argument('--no_viz', action='store_true',
                       help='Disable visualization generation')
    
    args = parser.parse_args()
    
    # Handle 'all' category
    if 'all' in args.category:
        args.category = ['Win32', 'Win64', 'APK', 'PDF', 'ELF', 'Net']
    
    # Handle string 'None' values for Random Forest
    if hasattr(args, 'rf_max_features') and args.rf_max_features == 'None':
        args.rf_max_features = None
    if hasattr(args, 'rf_class_weight') and args.rf_class_weight == 'None':
        args.rf_class_weight = None
    
    return args


def main():
    """Main function"""
    args = parse_arguments()
    
    # Check if required libraries are installed
    try:
        import lightgbm
        import xgboost
    except ImportError as e:
        print(f"âŒ Required library not installed: {e}")
        print("Please install with: pip install lightgbm xgboost scikit-learn pandas pyarrow")
        return
    
    # Initialize trainer
    trainer = MalwareModelTrainer(args)
    
    # Run training
    trainer.run_training()


if __name__ == "__main__":
    main()
