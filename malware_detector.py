#!/usr/bin/env python3
"""
EMBER2024 Malware Detector
Real-time malware detection using trained LightGBM/XGBoost models

Features:
- PE file analysis and feature extraction
- Real-time malware classification
- Confidence scoring and detailed analysis
- Support for single files or batch processing
- Comprehensive reporting and logging

Usage Examples:
python malware_detector.py --file suspicious_file.exe
python malware_detector.py --directory malware_samples/ --output results.json
python malware_detector.py --file sample.exe --model best_lightgbm_models/lightgbm_win32_model.txt
"""

import argparse
import json
import time
import hashlib
import struct
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime

# Core libraries
import numpy as np
import pandas as pd
import lightgbm as lgb
import xgboost as xgb

# PE analysis libraries
try:
    import pefile
    import entropy
    PE_ANALYSIS_AVAILABLE = True
except ImportError:
    PE_ANALYSIS_AVAILABLE = False
    print("Warning: pefile not available - install with: pip install pefile")

# Additional file format libraries
try:
    import zipfile
    import xml.etree.ElementTree as ET
    ARCHIVE_ANALYSIS_AVAILABLE = True
except ImportError:
    ARCHIVE_ANALYSIS_AVAILABLE = False
    print("Warning: zipfile/xml not available for APK analysis")

try:
    import magic
    MAGIC_AVAILABLE = True
except ImportError:
    MAGIC_AVAILABLE = False
    print("Warning: python-magic not available - install with: pip install python-magic")

# Suppress warnings
warnings.filterwarnings('ignore')

class EmberFeatureExtractor:
    """Extract EMBER-style features from various file types (PE, APK, ELF, .NET, PDF, PS1)"""
    
    def __init__(self):
        self.feature_names = []
        self._initialize_feature_names()
        
        # File type mappings
        self.file_type_signatures = {
            'PE': [b'MZ'],  # Windows PE files (Win32/Win64/.NET)
            'ELF': [b'\x7fELF'],  # Linux/Unix ELF files
            'APK': [b'PK\x03\x04'],  # Android APK (ZIP-based)
            'PDF': [b'%PDF'],  # PDF files
            'PS1': [b'#'],  # PowerShell scripts (simple heuristic)
        }
    
    def _initialize_feature_names(self):
        """Initialize the expected feature names based on EMBER format"""
        # Histogram features (256)
        self.feature_names.extend([f'histogram_{i}' for i in range(256)])
        
        # Byteentropy features (256) 
        self.feature_names.extend([f'byteentropy_{i}' for i in range(256)])
        
        # Entropy features (3) - computed from histogram/byteentropy for compatibility
        entropy_features = [
            'general_entropy', 'strings_entropy', 'section_overlay_entropy'
        ]
        self.feature_names.extend(entropy_features)
        
        print(f"Initialized {len(self.feature_names)} expected features (supporting all file types)")
    
    def detect_file_type(self, file_data: bytes) -> str:
        """Detect file type based on file signatures"""
        if len(file_data) < 4:
            return 'UNKNOWN'
        
        # Check signatures
        for file_type, signatures in self.file_type_signatures.items():
            for signature in signatures:
                if file_data.startswith(signature):
                    return file_type
        
        # Additional checks for specific types
        if b'dex\n' in file_data[:100]:  # Android DEX files
            return 'APK'
        
        if b'.NET' in file_data[:1024] or b'mscoree.dll' in file_data[:2048]:
            return 'DOTNET'
        
        # Check for PowerShell based on content patterns
        try:
            text_content = file_data[:1024].decode('utf-8', errors='ignore').lower()
            if any(ps_keyword in text_content for ps_keyword in 
                   ['param(', 'function ', '$_', 'powershell', 'cmdlet']):
                return 'PS1'
        except:
            pass
        
        return 'UNKNOWN'
    
    def extract_features(self, file_path: str) -> Dict:
        """Extract comprehensive features from any supported file type"""
        try:
            with open(file_path, 'rb') as f:
                file_data = f.read()
            
            features = {}
            
            # Basic file information
            features['file_size'] = len(file_data)
            features['md5'] = hashlib.md5(file_data).hexdigest()
            features['sha256'] = hashlib.sha256(file_data).hexdigest()
            
            # Detect file type
            file_type = self.detect_file_type(file_data)
            features['detected_file_type'] = file_type
            
            # Extract universal EMBER-style features (work for all file types)
            features.update(self._extract_histogram(file_data))
            features.update(self._extract_byteentropy(file_data))
            features.update(self._extract_minimal_entropy(file_data))
            
            # Extract file-type specific features
            if file_type == 'PE':
                features.update(self._extract_pe_features(file_path, file_data))
            elif file_type == 'APK':
                features.update(self._extract_apk_features(file_path, file_data))
            elif file_type == 'ELF':
                features.update(self._extract_elf_features(file_data))
            elif file_type == 'DOTNET':
                features.update(self._extract_dotnet_features(file_path, file_data))
            elif file_type == 'PDF':
                features.update(self._extract_pdf_features(file_data))
            elif file_type == 'PS1':
                features.update(self._extract_ps1_features(file_data))
            else:
                features.update(self._extract_generic_features(file_data))
            
            return features
            
        except Exception as e:
            print(f"Error extracting features from {file_path}: {e}")
            return self._get_default_features()
    
    def _extract_histogram(self, file_data: bytes) -> Dict:
        """Extract byte histogram (256 features) - raw counts, not normalized"""
        histogram = [0] * 256
        
        if file_data:
            for byte in file_data:
                histogram[byte] += 1
        
        # Return raw counts (not normalized frequencies)
        return {f'histogram_{i}': histogram[i] for i in range(256)}
    
    def _extract_byteentropy(self, file_data: bytes, window_size: int = 2048) -> Dict:
        """Extract byte entropy in sliding windows (256 features)
        Based on training data, these appear to be raw entropy values scaled up
        """
        if len(file_data) < window_size:
            # For small files, pad the result with zeros
            return {f'byteentropy_{i}': 0.0 for i in range(256)}
        
        entropies = []
        
        # Calculate entropy for non-overlapping windows
        for i in range(0, len(file_data), window_size):
            window = file_data[i:i + window_size]
            if len(window) == window_size:  # Only use full windows
                entropy_val = self._calculate_entropy(window)
                # Scale entropy to match training data range (observed: 0 to ~22M)
                # Training data seems to have scaled entropy values
                scaled_entropy = entropy_val * window_size  # Scale by window size
                entropies.append(scaled_entropy)
        
        # Pad or truncate to exactly 256 values
        while len(entropies) < 256:
            entropies.append(0.0)
        
        entropies = entropies[:256]
        
        return {f'byteentropy_{i}': entropies[i] for i in range(256)}
    
    def _extract_minimal_entropy(self, file_data: bytes) -> Dict:
        """Extract minimal entropy features computed from histogram data"""
        # Compute entropy from histogram for compatibility but minimal impact
        histogram = [0] * 256
        
        if file_data:
            for byte in file_data:
                histogram[byte] += 1
            
            # Calculate entropy from histogram
            total = len(file_data)
            entropy_val = 0.0
            if total > 0:
                for count in histogram:
                    if count > 0:
                        p = count / total
                        entropy_val -= p * np.log2(p)
        else:
            entropy_val = 0.0
        
        # Return same entropy for all three features to minimize their influence
        return {
            'general_entropy': entropy_val,
            'strings_entropy': entropy_val,  # Same as general
            'section_overlay_entropy': entropy_val  # Same as general
        }
    
    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data"""
        if not data:
            return 0.0
        
        # Count byte frequencies
        byte_counts = [0] * 256
        for byte in data:
            byte_counts[byte] += 1
        
        # Calculate entropy
        entropy_val = 0.0
        total = len(data)
        
        for count in byte_counts:
            if count > 0:
                p = count / total
                entropy_val -= p * np.log2(p)
        
        return entropy_val
    
    def _extract_pe_features(self, file_path: str, file_data: bytes) -> Dict:
        """Extract PE-specific features using pefile"""
        features = {}
        
        try:
            pe = pefile.PE(file_path)
            
            # PE header information
            features['pe_machine'] = pe.FILE_HEADER.Machine
            features['pe_characteristics'] = pe.FILE_HEADER.Characteristics
            features['pe_subsystem'] = pe.OPTIONAL_HEADER.Subsystem
            features['pe_dll_characteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics
            
            # Section information
            features['pe_num_sections'] = pe.FILE_HEADER.NumberOfSections
            
            # Entry point entropy
            try:
                entry_point = pe.OPTIONAL_HEADER.AddressOfEntryPoint
                for section in pe.sections:
                    if (entry_point >= section.VirtualAddress and 
                        entry_point < section.VirtualAddress + section.Misc_VirtualSize):
                        section_data = section.get_data()
                        features['entry_point_entropy'] = self._calculate_entropy(section_data)
                        break
                else:
                    features['entry_point_entropy'] = 0.0
            except:
                features['entry_point_entropy'] = 0.0
            
            pe.close()
            
        except Exception as e:
            print(f"Warning: Could not parse PE file {file_path}: {e}")
            # Set default values
            features.update({
                'pe_machine': 0,
                'pe_characteristics': 0,
                'pe_subsystem': 0,
                'pe_dll_characteristics': 0,
                'pe_num_sections': 0,
                'entry_point_entropy': 0.0
            })
        
        return features
    
    def _extract_apk_features(self, file_path: str, file_data: bytes) -> Dict:
        """Extract APK-specific features"""
        features = {
            'apk_num_files': 0,
            'apk_has_manifest': 0,
            'apk_has_dex': 0,
            'apk_num_permissions': 0,
            'apk_compressed_ratio': 0.0
        }
        
        if not ARCHIVE_ANALYSIS_AVAILABLE:
            return features
        
        try:
            with zipfile.ZipFile(file_path, 'r') as apk:
                file_list = apk.namelist()
                features['apk_num_files'] = len(file_list)
                
                # Check for key APK components
                features['apk_has_manifest'] = 1 if 'AndroidManifest.xml' in file_list else 0
                features['apk_has_dex'] = 1 if any('classes.dex' in f for f in file_list) else 0
                
                # Calculate compression ratio
                total_compressed = sum(apk.getinfo(f).compress_size for f in file_list)
                total_uncompressed = sum(apk.getinfo(f).file_size for f in file_list)
                if total_uncompressed > 0:
                    features['apk_compressed_ratio'] = total_compressed / total_uncompressed
                
                # Try to extract permissions from manifest
                try:
                    manifest_data = apk.read('AndroidManifest.xml')
                    # Simple permission counting (rough heuristic)
                    permission_count = manifest_data.count(b'permission')
                    features['apk_num_permissions'] = permission_count
                except:
                    pass
                    
        except Exception as e:
            print(f"Warning: Could not analyze APK structure: {e}")
        
        return features
    
    def _extract_elf_features(self, file_data: bytes) -> Dict:
        """Extract ELF-specific features"""
        features = {
            'elf_class': 0,  # 32-bit or 64-bit
            'elf_data': 0,   # Endianness
            'elf_machine': 0,  # Architecture
            'elf_num_sections': 0,
            'elf_has_dynamic': 0
        }
        
        try:
            if len(file_data) < 64:  # Minimum ELF header size
                return features
            
            # Parse ELF header
            elf_header = file_data[:64]
            
            # ELF class (32/64 bit)
            features['elf_class'] = elf_header[4]
            
            # Data encoding (endianness)
            features['elf_data'] = elf_header[5]
            
            # Machine architecture (bytes 18-19)
            features['elf_machine'] = struct.unpack('<H', elf_header[18:20])[0]
            
            # Section header information
            e_shoff = struct.unpack('<Q', elf_header[40:48])[0] if features['elf_class'] == 2 else struct.unpack('<I', elf_header[32:36])[0]
            e_shnum = struct.unpack('<H', elf_header[60:62])[0]
            features['elf_num_sections'] = e_shnum
            
            # Check for dynamic linking
            features['elf_has_dynamic'] = 1 if b'.dynamic' in file_data else 0
            
        except Exception as e:
            print(f"Warning: Could not parse ELF header: {e}")
        
        return features
    
    def _extract_dotnet_features(self, file_path: str, file_data: bytes) -> Dict:
        """Extract .NET-specific features"""
        features = {
            'dotnet_has_clr': 0,
            'dotnet_num_streams': 0,
            'dotnet_has_strong_name': 0,
            'dotnet_version_major': 0
        }
        
        try:
            # Check for .NET CLR header
            features['dotnet_has_clr'] = 1 if b'mscoree.dll' in file_data else 0
            
            # Look for .NET metadata signatures
            if b'#~' in file_data or b'#-' in file_data:
                features['dotnet_num_streams'] = file_data.count(b'#~') + file_data.count(b'#-')
            
            # Check for strong name signature
            features['dotnet_has_strong_name'] = 1 if b'StrongNameSignature' in file_data else 0
            
            # Try to extract version info (simple heuristic)
            if b'v4.0' in file_data:
                features['dotnet_version_major'] = 4
            elif b'v2.0' in file_data:
                features['dotnet_version_major'] = 2
            elif b'v1.1' in file_data:
                features['dotnet_version_major'] = 1
            
            # Also extract PE features since .NET files are PE files
            if PE_ANALYSIS_AVAILABLE:
                pe_features = self._extract_pe_features(file_path, file_data)
                features.update(pe_features)
                
        except Exception as e:
            print(f"Warning: Could not analyze .NET features: {e}")
        
        return features
    
    def _extract_pdf_features(self, file_data: bytes) -> Dict:
        """Extract PDF-specific features"""
        features = {
            'pdf_num_objects': 0,
            'pdf_num_streams': 0,
            'pdf_has_javascript': 0,
            'pdf_has_forms': 0,
            'pdf_num_pages': 0
        }
        
        try:
            # Count PDF objects
            features['pdf_num_objects'] = file_data.count(b'obj')
            
            # Count streams
            features['pdf_num_streams'] = file_data.count(b'stream')
            
            # Check for JavaScript
            js_keywords = [b'/JavaScript', b'/JS', b'eval(', b'unescape(']
            features['pdf_has_javascript'] = 1 if any(keyword in file_data for keyword in js_keywords) else 0
            
            # Check for forms
            form_keywords = [b'/AcroForm', b'/XFA', b'/Fields']
            features['pdf_has_forms'] = 1 if any(keyword in file_data for keyword in form_keywords) else 0
            
            # Try to count pages (rough estimate)
            features['pdf_num_pages'] = file_data.count(b'/Type/Page')
            
        except Exception as e:
            print(f"Warning: Could not analyze PDF features: {e}")
        
        return features
    
    def _extract_ps1_features(self, file_data: bytes) -> Dict:
        """Extract PowerShell script features"""
        features = {
            'ps1_num_cmdlets': 0,
            'ps1_has_obfuscation': 0,
            'ps1_has_download': 0,
            'ps1_has_exec': 0,
            'ps1_entropy': 0.0
        }
        
        try:
            # Convert to text for analysis
            text_content = file_data.decode('utf-8', errors='ignore').lower()
            
            # Count common PowerShell cmdlets
            cmdlets = ['get-', 'set-', 'new-', 'invoke-', 'start-', 'stop-']
            features['ps1_num_cmdlets'] = sum(text_content.count(cmdlet) for cmdlet in cmdlets)
            
            # Check for obfuscation indicators
            obfuscation_indicators = ['[char]', 'iex', 'invoke-expression', '$env:', '-enc', 'frombase64']
            features['ps1_has_obfuscation'] = 1 if any(indicator in text_content for indicator in obfuscation_indicators) else 0
            
            # Check for download capabilities
            download_indicators = ['downloadstring', 'webclient', 'wget', 'curl', 'invoke-webrequest']
            features['ps1_has_download'] = 1 if any(indicator in text_content for indicator in download_indicators) else 0
            
            # Check for execution capabilities
            exec_indicators = ['invoke-expression', 'iex', 'start-process', '.exe', 'cmd.exe']
            features['ps1_has_exec'] = 1 if any(indicator in text_content for indicator in exec_indicators) else 0
            
            # Calculate text entropy
            features['ps1_entropy'] = self._calculate_entropy(file_data)
            
        except Exception as e:
            print(f"Warning: Could not analyze PowerShell features: {e}")
        
        return features
    
    def _extract_generic_features(self, file_data: bytes) -> Dict:
        """Extract generic features for unknown file types"""
        features = {
            'generic_printable_ratio': 0.0,
            'generic_null_ratio': 0.0,
            'generic_entropy': 0.0,
            'generic_longest_string': 0
        }
        
        try:
            if len(file_data) > 0:
                # Calculate printable character ratio
                printable_count = sum(1 for b in file_data if 32 <= b <= 126)
                features['generic_printable_ratio'] = printable_count / len(file_data)
                
                # Calculate null byte ratio
                null_count = file_data.count(0)
                features['generic_null_ratio'] = null_count / len(file_data)
                
                # Calculate overall entropy
                features['generic_entropy'] = self._calculate_entropy(file_data)
                
                # Find longest printable string
                current_length = 0
                max_length = 0
                for b in file_data:
                    if 32 <= b <= 126:  # Printable ASCII
                        current_length += 1
                        max_length = max(max_length, current_length)
                    else:
                        current_length = 0
                features['generic_longest_string'] = max_length
                
        except Exception as e:
            print(f"Warning: Could not extract generic features: {e}")
        
        return features
    
    def _get_default_features(self) -> Dict:
        """Return default feature values when extraction fails"""
        features = {}
        
        # Default histogram (zero counts)
        for i in range(256):
            features[f'histogram_{i}'] = 0
        
        # Default byteentropy (zero entropy)
        for i in range(256):
            features[f'byteentropy_{i}'] = 0.0
        
        # Default entropy values (minimal impact)
        features.update({
            'general_entropy': 0.0,
            'strings_entropy': 0.0,
            'section_overlay_entropy': 0.0
        })
        
        return features
    
    def prepare_for_prediction(self, features: Dict) -> np.ndarray:
        """Prepare extracted features for model prediction"""
        # Create feature vector in the expected order
        feature_vector = []
        
        for feature_name in self.feature_names:
            feature_vector.append(features.get(feature_name, 0.0))
        
        return np.array(feature_vector).reshape(1, -1)


class MalwareDetector:
    """Main malware detection engine"""
    
    def __init__(self, model_path: str, model_type: str = 'auto'):
        self.model_path = Path(model_path)
        self.model_type = self._detect_model_type(model_type)
        self.model = None
        self.feature_extractor = EmberFeatureExtractor()
        
        # Load the model
        self._load_model()
        
        # Detection thresholds
        self.malware_threshold = 0.5
        self.high_confidence_threshold = 0.8
        self.low_confidence_threshold = 0.3
    
    def _detect_model_type(self, model_type: str) -> str:
        """Auto-detect model type from file extension"""
        if model_type != 'auto':
            return model_type
        
        if self.model_path.suffix == '.txt':
            return 'lightgbm'
        elif self.model_path.suffix in ['.json', '.model']:
            return 'xgboost'
        else:
            raise ValueError(f"Cannot determine model type from {self.model_path}")
    
    def _load_model(self):
        """Load the trained model"""
        print(f"Loading {self.model_type} model from {self.model_path}")
        
        try:
            if self.model_type == 'lightgbm':
                self.model = lgb.Booster(model_file=str(self.model_path))
            elif self.model_type == 'xgboost':
                self.model = xgb.Booster()
                self.model.load_model(str(self.model_path))
            else:
                raise ValueError(f"Unsupported model type: {self.model_type}")
            
            print(f"‚úì Model loaded successfully")
            
        except Exception as e:
            raise Exception(f"Failed to load model: {e}")
    
    def predict_file(self, file_path: str) -> Dict:
        """Analyze a single file and return detection results"""
        start_time = time.time()
        
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        print(f"\nüîç Analyzing: {file_path.name}")
        
        # Extract features
        print("  Extracting features...")
        features = self.feature_extractor.extract_features(str(file_path))
        
        # Prepare features for prediction
        feature_vector = self.feature_extractor.prepare_for_prediction(features)
        
        # Make prediction
        print("  Making prediction...")
        if self.model_type == 'lightgbm':
            prediction_proba = self.model.predict(feature_vector)[0]
        else:  # xgboost
            dtest = xgb.DMatrix(feature_vector)
            prediction_proba = self.model.predict(dtest)[0]
        
        prediction = int(prediction_proba > self.malware_threshold)
        
        # Calculate analysis time
        analysis_time = time.time() - start_time
        
        # Generate detailed results
        result = self._generate_result(
            file_path=str(file_path),
            features=features,
            prediction=prediction,
            confidence=prediction_proba,
            analysis_time=analysis_time
        )
        
        return result
    
    def predict_directory(self, directory_path: str, pattern: str = "*.exe") -> List[Dict]:
        """Analyze all files in a directory"""
        directory = Path(directory_path)
        if not directory.exists():
            raise FileNotFoundError(f"Directory not found: {directory}")
        
        files = list(directory.glob(pattern))
        if not files:
            print(f"No files matching '{pattern}' found in {directory}")
            return []
        
        print(f"\nüìÅ Analyzing {len(files)} files from {directory}")
        
        results = []
        for i, file_path in enumerate(files, 1):
            print(f"\n[{i}/{len(files)}] Processing {file_path.name}")
            
            try:
                result = self.predict_file(str(file_path))
                results.append(result)
                
                # Show progress
                status = "ü¶† MALWARE" if result['analysis_results']['prediction'] == 1 else "‚úÖ BENIGN"
                confidence = result['analysis_results']['confidence']
                file_type = result['feature_analysis'].get('file_type', 'UNKNOWN')
                print(f"  Result: {status} [{file_type}] (confidence: {confidence:.3f})")
                
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                error_result = {
                    'file_path': str(file_path),
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
                results.append(error_result)
        
        return results
    
    def _generate_result(self, file_path: str, features: Dict, prediction: int, 
                        confidence: float, analysis_time: float) -> Dict:
        """Generate comprehensive analysis result"""
        
        # Determine confidence level
        if confidence >= self.high_confidence_threshold:
            confidence_level = "HIGH"
        elif confidence <= self.low_confidence_threshold:
            confidence_level = "LOW"
        else:
            confidence_level = "MEDIUM"
        
        # Determine threat level
        if prediction == 1:
            if confidence >= 0.9:
                threat_level = "CRITICAL"
            elif confidence >= 0.7:
                threat_level = "HIGH"
            else:
                threat_level = "MEDIUM"
        else:
            threat_level = "NONE"
        
        # Basic file info
        file_info = {
            'file_path': file_path,
            'file_name': Path(file_path).name,
            'file_size': features.get('file_size', 0),
            'md5': features.get('md5', ''),
            'sha256': features.get('sha256', '')
        }
        
        # Analysis results
        analysis_results = {
            'prediction': prediction,
            'prediction_label': 'MALWARE' if prediction == 1 else 'BENIGN',
            'confidence': float(confidence),
            'confidence_level': confidence_level,
            'threat_level': threat_level,
            'analysis_time': analysis_time
        }
        
        # Feature analysis
        feature_analysis = {
            'file_type': features.get('detected_file_type', 'UNKNOWN'),
            'file_size_mb': features.get('file_size', 0) / (1024 * 1024),
            'suspicious_indicators': self._identify_suspicious_indicators(features, confidence)
        }
        
        # File type specific analysis
        file_type_analysis = {}
        file_type = features.get('detected_file_type', 'UNKNOWN')
        
        if file_type == 'APK':
            file_type_analysis = {
                'num_files': features.get('apk_num_files', 0),
                'has_manifest': features.get('apk_has_manifest', 0),
                'compression_ratio': features.get('apk_compressed_ratio', 0.0),
                'num_permissions': features.get('apk_num_permissions', 0)
            }
        elif file_type == 'ELF':
            file_type_analysis = {
                'architecture': features.get('elf_machine', 0),
                'class': '64-bit' if features.get('elf_class', 0) == 2 else '32-bit',
                'num_sections': features.get('elf_num_sections', 0),
                'has_dynamic': features.get('elf_has_dynamic', 0)
            }
        elif file_type == 'DOTNET':
            file_type_analysis = {
                'has_clr': features.get('dotnet_has_clr', 0),
                'version_major': features.get('dotnet_version_major', 0),
                'has_strong_name': features.get('dotnet_has_strong_name', 0),
                'num_streams': features.get('dotnet_num_streams', 0)
            }
        elif file_type == 'PDF':
            file_type_analysis = {
                'num_objects': features.get('pdf_num_objects', 0),
                'num_streams': features.get('pdf_num_streams', 0),
                'has_javascript': features.get('pdf_has_javascript', 0),
                'has_forms': features.get('pdf_has_forms', 0),
                'num_pages': features.get('pdf_num_pages', 0)
            }
        elif file_type == 'PS1':
            file_type_analysis = {
                'num_cmdlets': features.get('ps1_num_cmdlets', 0),
                'has_obfuscation': features.get('ps1_has_obfuscation', 0),
                'has_download': features.get('ps1_has_download', 0),
                'has_exec': features.get('ps1_has_exec', 0),
                'entropy': features.get('ps1_entropy', 0.0)
            }
        
        # PE analysis (if available) - for PE, DOTNET files
        pe_analysis = {}
        if 'pe_machine' in features:
            pe_analysis = {
                'machine_type': features.get('pe_machine', 0),
                'num_sections': features.get('pe_num_sections', 0),
                'entry_point_entropy': features.get('entry_point_entropy', 0.0),
                'subsystem': features.get('pe_subsystem', 0)
            }
        
        return {
            'timestamp': datetime.now().isoformat(),
            'model_type': self.model_type,
            'model_path': str(self.model_path),
            'file_info': file_info,
            'analysis_results': analysis_results,
            'feature_analysis': feature_analysis,
            'file_type_analysis': file_type_analysis,
            'pe_analysis': pe_analysis
        }
    
    def _identify_suspicious_indicators(self, features: Dict, confidence: float) -> List[str]:
        """Identify suspicious indicators across all file types"""
        indicators = []
        
        # Unusual file size
        file_size = features.get('file_size', 0)
        if file_size < 10000:
            indicators.append("Very small file size - possible dropper")
        elif file_size > 50000000:  # 50MB
            indicators.append("Very large file size - unusual for typical files")
        
        # Analyze histogram distribution
        histogram_values = [features.get(f'histogram_{i}', 0) for i in range(256)]
        if histogram_values:
            max_freq = max(histogram_values)
            if max_freq > 0.5:
                indicators.append("Highly skewed byte distribution - possible padding")
            
            # Check for uniform distribution (possible encryption)
            std_dev = np.std(histogram_values)
            if std_dev < 0.001:
                indicators.append("Uniform byte distribution - possible encryption")
        
        # Analyze byteentropy patterns
        entropy_values = [features.get(f'byteentropy_{i}', 0) for i in range(256)]
        if entropy_values:
            avg_entropy = np.mean(entropy_values)
            if avg_entropy > 7.0:
                indicators.append(f"High average entropy ({avg_entropy:.2f}) - possible packing")
            elif avg_entropy < 1.0:
                indicators.append(f"Low average entropy ({avg_entropy:.2f}) - possible simple structure")
        
        # File type specific indicators
        file_type = features.get('detected_file_type', 'UNKNOWN')
        
        if file_type == 'PE':
            # PE-specific indicators
            if 'pe_num_sections' in features:
                num_sections = features['pe_num_sections']
                if num_sections > 10:
                    indicators.append(f"High number of PE sections ({num_sections})")
                elif num_sections < 2:
                    indicators.append(f"Unusual number of PE sections ({num_sections})")
            
            # Entry point entropy
            if features.get('entry_point_entropy', 0) > 7.0:
                indicators.append("High entry point entropy - possible packing")
                
        elif file_type == 'APK':
            # APK-specific indicators
            if features.get('apk_num_files', 0) > 1000:
                indicators.append(f"Large number of files in APK ({features['apk_num_files']})")
            if features.get('apk_has_manifest', 0) == 0:
                indicators.append("Missing AndroidManifest.xml - suspicious APK structure")
            if features.get('apk_compressed_ratio', 0) < 0.3:
                indicators.append("Low compression ratio - possible encrypted content")
                
        elif file_type == 'ELF':
            # ELF-specific indicators
            if features.get('elf_num_sections', 0) > 50:
                indicators.append(f"High number of ELF sections ({features['elf_num_sections']})")
            if features.get('elf_has_dynamic', 0) == 0:
                indicators.append("Statically linked ELF - possible malware technique")
                
        elif file_type == 'DOTNET':
            # .NET-specific indicators
            if features.get('dotnet_has_strong_name', 0) == 0:
                indicators.append("Missing strong name signature - unsigned .NET assembly")
            if features.get('dotnet_has_clr', 0) == 0:
                indicators.append("Missing CLR header - suspicious .NET file")
                
        elif file_type == 'PDF':
            # PDF-specific indicators
            if features.get('pdf_has_javascript', 0) == 1:
                indicators.append("Contains JavaScript - potential PDF exploit")
            if features.get('pdf_num_streams', 0) > 100:
                indicators.append(f"High number of streams ({features['pdf_num_streams']}) - possible obfuscation")
            if features.get('pdf_has_forms', 0) == 1:
                indicators.append("Contains forms - potential data harvesting")
                
        elif file_type == 'PS1':
            # PowerShell-specific indicators
            if features.get('ps1_has_obfuscation', 0) == 1:
                indicators.append("Contains obfuscation techniques")
            if features.get('ps1_has_download', 0) == 1:
                indicators.append("Contains download capabilities - possible dropper")
            if features.get('ps1_has_exec', 0) == 1:
                indicators.append("Contains execution commands - potential malware")
            if features.get('ps1_entropy', 0) > 6.0:
                indicators.append(f"High script entropy ({features['ps1_entropy']:.2f}) - possible obfuscation")
                
        elif file_type == 'UNKNOWN':
            # Generic file indicators
            printable_ratio = features.get('generic_printable_ratio', 0)
            if printable_ratio < 0.1:
                indicators.append(f"Low printable content ({printable_ratio:.2f}) - possible binary obfuscation")
            elif printable_ratio > 0.95:
                indicators.append(f"Very high printable content ({printable_ratio:.2f}) - possible script")
            
            null_ratio = features.get('generic_null_ratio', 0)
            if null_ratio > 0.5:
                indicators.append(f"High null byte ratio ({null_ratio:.2f}) - possible padding")
        
        # Model confidence indicators
        if confidence > 0.95:
            indicators.append("Very high model confidence")
        elif 0.4 < confidence < 0.6:
            indicators.append("Borderline classification - manual review recommended")
        
        return indicators
    
    def generate_report(self, results: Union[Dict, List[Dict]], output_path: Optional[str] = None) -> str:
        """Generate a comprehensive analysis report"""
        if isinstance(results, dict):
            results = [results]
        
        # Calculate summary statistics
        total_files = len(results)
        malware_count = sum(1 for r in results if r.get('analysis_results', {}).get('prediction') == 1)
        benign_count = total_files - malware_count
        
        # Generate report
        report_lines = [
            "EMBER2024 Malware Detection Report",
            "=" * 50,
            f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Model Type: {self.model_type}",
            f"Model Path: {self.model_path}",
            "",
            "SUMMARY:",
            f"  Total Files Analyzed: {total_files}",
            f"  Malware Detected: {malware_count}",
            f"  Benign Files: {benign_count}",
            f"  Detection Rate: {(malware_count/total_files)*100:.1f}%",
            "",
            "DETAILED RESULTS:",
            "-" * 50
        ]
        
        # Add detailed results for each file
        for i, result in enumerate(results, 1):
            if 'error' in result:
                report_lines.extend([
                    f"{i}. {result['file_path']}",
                    f"   ‚ùå ERROR: {result['error']}",
                    ""
                ])
                continue
            
            file_info = result['file_info']
            analysis = result['analysis_results']
            feature_analysis = result['feature_analysis']
            file_type_analysis = result.get('file_type_analysis', {})
            
            status_icon = "ü¶†" if analysis['prediction'] == 1 else "‚úÖ"
            file_type = feature_analysis.get('file_type', 'UNKNOWN')
            
            report_lines.extend([
                f"{i}. {file_info['file_name']} {status_icon} [{file_type}]",
                f"   Path: {file_info['file_path']}",
                f"   Result: {analysis['prediction_label']}",
                f"   Confidence: {analysis['confidence']:.3f} ({analysis['confidence_level']})",
                f"   Threat Level: {analysis['threat_level']}",
                f"   File Size: {feature_analysis['file_size_mb']:.2f} MB",
                f"   Analysis Time: {analysis['analysis_time']:.2f}s"
            ])
            
            # Add file type specific information
            if file_type_analysis:
                report_lines.append("   File Type Analysis:")
                for key, value in file_type_analysis.items():
                    report_lines.append(f"     {key.replace('_', ' ').title()}: {value}")
            
            # Add suspicious indicators
            indicators = feature_analysis['suspicious_indicators']
            if indicators:
                report_lines.append("   Suspicious Indicators:")
                for indicator in indicators:
                    report_lines.append(f"     ‚Ä¢ {indicator}")
            
            # Add PE information if available
            pe_analysis = result.get('pe_analysis', {})
            if pe_analysis:
                report_lines.extend([
                    f"   PE Sections: {pe_analysis.get('num_sections', 'N/A')}",
                    f"   Entry Point Entropy: {pe_analysis.get('entry_point_entropy', 0):.2f}"
                ])
            
            report_lines.append("")
        
        # Add threat level summary
        threat_levels = {}
        for result in results:
            if 'analysis_results' in result:
                level = result['analysis_results']['threat_level']
                threat_levels[level] = threat_levels.get(level, 0) + 1
        
        if threat_levels:
            report_lines.extend([
                "THREAT LEVEL DISTRIBUTION:",
                "-" * 30
            ])
            for level, count in sorted(threat_levels.items()):
                report_lines.append(f"  {level}: {count} files")
        
        report_text = "\n".join(report_lines)
        
        # Save to file if output path provided
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"\nüìÑ Report saved to: {output_path}")
        
        return report_text


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="EMBER2024 Malware Detector",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # Input options
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--file', type=str, help='Single file to analyze')
    group.add_argument('--directory', type=str, help='Directory to analyze')
    
    # Model options
    parser.add_argument('--model', type=str, required=True,
                       help='Path to trained model file (.txt for LightGBM, .json for XGBoost)')
    parser.add_argument('--model_type', choices=['lightgbm', 'xgboost', 'auto'], 
                       default='auto', help='Model type (auto-detect by default)')
    
    # Analysis options
    parser.add_argument('--pattern', type=str, default='*',
                       help='File pattern for directory analysis (e.g., *.exe, *.apk, *.pdf, *.ps1)')
    parser.add_argument('--threshold', type=float, default=0.5,
                       help='Malware detection threshold')
    
    # Output options
    parser.add_argument('--output', type=str, help='Output file for results (JSON format)')
    parser.add_argument('--report', type=str, help='Output file for text report')
    parser.add_argument('--verbose', action='store_true', help='Verbose output')
    
    return parser.parse_args()


def main():
    """Main function"""
    args = parse_arguments()
    
    print("üõ°Ô∏è  EMBER2024 Malware Detector")
    print("=" * 50)
    
    # Check if model exists
    model_path = Path(args.model)
    if not model_path.exists():
        print(f"‚ùå Model file not found: {model_path}")
        return
    
    # Initialize detector
    try:
        detector = MalwareDetector(str(model_path), args.model_type)
        detector.malware_threshold = args.threshold
    except Exception as e:
        print(f"‚ùå Failed to initialize detector: {e}")
        return
    
    # Analyze files
    results = []
    
    try:
        if args.file:
            # Single file analysis
            result = detector.predict_file(args.file)
            results = [result]
            
            # Print immediate result
            analysis = result['analysis_results']
            status = "ü¶† MALWARE" if analysis['prediction'] == 1 else "‚úÖ BENIGN"
            print(f"\nüîç RESULT: {status}")
            print(f"   Confidence: {analysis['confidence']:.3f} ({analysis['confidence_level']})")
            print(f"   Threat Level: {analysis['threat_level']}")
            print(f"   Analysis Time: {analysis['analysis_time']:.2f}s")
            
        else:
            # Directory analysis
            results = detector.predict_directory(args.directory, args.pattern)
        
        # Generate summary
        if results:
            total_files = len(results)
            malware_count = sum(1 for r in results if r.get('analysis_results', {}).get('prediction') == 1)
            
            print(f"\nüìä ANALYSIS COMPLETE:")
            print(f"   Files Analyzed: {total_files}")
            print(f"   Malware Detected: {malware_count}")
            print(f"   Benign Files: {total_files - malware_count}")
            print(f"   Detection Rate: {(malware_count/total_files)*100:.1f}%")
        
        # Save results to JSON
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2)
            print(f"\nüíæ Results saved to: {args.output}")
        
        # Generate text report
        if args.report:
            detector.generate_report(results, args.report)
        elif args.verbose:
            # Print report to console
            report = detector.generate_report(results)
            print(f"\n{report}")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Analysis interrupted by user")
    except Exception as e:
        print(f"‚ùå Analysis failed: {e}")


if __name__ == "__main__":
    main()
