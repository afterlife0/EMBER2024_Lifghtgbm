EMBER2024 Malware Detection Training Commands Guide
==================================================
Generated: August 28, 2025
Author: EMBER2024 Training Pipeline
Version: 1.0

OVERVIEW
========
This guide provides optimal training commands for different scenarios using the EMBER2024 
malware detection dataset. All commands are designed to prevent overfitting while maintaining
high performance across different file types and use cases.

Dataset Information:
- Win32: 3,120,000 train + 720,000 test samples
- Win64: 1,040,000 train + 240,000 test samples  
- .NET: 520,000 train + 120,000 test samples
- APK: 416,000 train + 96,000 test samples
- PDF: 104,000 train + 24,000 test samples
- ELF: 52,000 train + 12,000 test samples

===============================================================================
1. OPTIMAL TRAINING COMMANDS
===============================================================================

1.1 PRODUCTION-READY TRAINING (RECOMMENDED - ANTI-OVERFITTING)
-------------------------------------------------------------
Purpose: Best balance of performance and generalization with overfitting prevention
Dataset: Win32 (largest, most diverse)
Sample Size: 1,000,000 samples
Models: LightGBM, XGBoost, and Random Forest

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_ultra_focused_features --output_dir optimal_models --sample_size 1000000 --models lightgbm xgboost randomforest --cv_folds 5 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_n_estimators 1500 --lgb_num_leaves 31 --lgb_learning_rate 0.03 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_bagging_freq 5 --lgb_min_child_samples 100 --xgb_n_estimators 1500 --xgb_max_depth 4 --xgb_learning_rate 0.03 --xgb_subsample 0.7 --xgb_colsample_bytree 0.7 --xgb_min_child_weight 5 --xgb_reg_alpha 0.2 --xgb_reg_lambda 2.0 --rf_n_estimators 300 --rf_max_depth 10 --rf_min_samples_split 10 --rf_min_samples_leaf 5

Expected Training Time: 15-20 minutes
Expected Performance: >93% AUC with excellent generalization and reduced overfitting

1.2 MULTI-CATEGORY COMPREHENSIVE TRAINING (ANTI-OVERFITTING)
-----------------------------------------------------------
Purpose: Train on multiple file types for broader coverage with overfitting prevention
Dataset: Win32 + Win64 + APK (covers Windows and Android)
Sample Size: 1,500,000 samples (distributed across categories)

Command:
python train_malware_models.py --category Win32 Win64 APK --data_dir ember2024_ultra_focused_features --output_dir multi_category_models --sample_size 1500000 --models lightgbm xgboost randomforest --cv_folds 5 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_learning_rate 0.03 --lgb_num_leaves 31 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_min_child_samples 100 --xgb_learning_rate 0.03 --xgb_max_depth 4 --xgb_subsample 0.7 --xgb_colsample_bytree 0.7 --xgb_reg_alpha 0.2 --xgb_reg_lambda 2.0 --rf_n_estimators 300 --rf_max_depth 10

Expected Training Time: 20-25 minutes
Coverage: Windows PE files and Android APK files with reduced overfitting

1.3 ALL CATEGORIES TRAINING (ANTI-OVERFITTING)
----------------------------------------------
Purpose: Maximum coverage across all file types with overfitting prevention
Dataset: All 6 categories (Win32, Win64, APK, PDF, ELF, .NET)
Sample Size: 2,000,000 samples (smart distributed)

Command:
python train_malware_models.py --category all --data_dir ember2024_ultra_focused_features --output_dir universal_models --sample_size 2000000 --models lightgbm xgboost randomforest --cv_folds 5 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_learning_rate 0.03 --lgb_num_leaves 31 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_min_child_samples 100 --xgb_learning_rate 0.03 --xgb_max_depth 4 --xgb_subsample 0.7 --xgb_colsample_bytree 0.7 --xgb_reg_alpha 0.2 --xgb_reg_lambda 2.0 --rf_n_estimators 300 --rf_max_depth 10

Expected Training Time: 25-35 minutes
Coverage: Universal malware detector for all file types with excellent generalization

1.4 CONSERVATIVE TRAINING (MAXIMUM ANTI-OVERFITTING) ⭐
------------------------------------------------------
Purpose: Minimize overfitting for deployment in production environments
Dataset: Win32 (largest dataset for stable training)
Sample Size: 1,500,000 samples
Models: LightGBM, XGBoost, and Random Forest with conservative parameters

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_ultra_focused_features --output_dir conservative_models --sample_size 1500000 --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.3 --early_stopping_rounds 20 --random_state 42 --lgb_n_estimators 1000 --lgb_num_leaves 15 --lgb_learning_rate 0.02 --lgb_feature_fraction 0.6 --lgb_bagging_fraction 0.6 --lgb_bagging_freq 7 --lgb_min_child_samples 150 --xgb_n_estimators 1000 --xgb_max_depth 3 --xgb_learning_rate 0.02 --xgb_subsample 0.6 --xgb_colsample_bytree 0.6 --xgb_min_child_weight 10 --xgb_reg_alpha 0.5 --xgb_reg_lambda 5.0 --rf_n_estimators 200 --rf_max_depth 8 --rf_min_samples_split 20 --rf_min_samples_leaf 10

Performance Features:
- Ultra-low learning rates (0.02): Maximum stability
- Conservative tree complexity: Prevents overfitting
- Strong regularization: L1=0.5, L2=5.0 for XGBoost
- Aggressive feature subsampling: 60% feature usage
- Large validation set (30%): Robust evaluation
- 7-fold CV: Comprehensive performance assessment
- Early stopping (20 rounds): Prevents overtraining

Expected Training Time: 20-25 minutes
Expected Performance: >91% AUC with maximum generalization
Memory Usage: 10-12 GB
Best for: Production deployment requiring maximum stability

1.5 CATEGORY-SPECIFIC ANTI-OVERFITTING TRAINING ⭐
--------------------------------------------------

1.5.1 WIN32 CONSERVATIVE TRAINING
Command:
python train_malware_models.py --category Win32 --data_dir ember2024_ultra_focused_features --output_dir conservative_win32 --sample_size 1500000 --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_n_estimators 1500 --lgb_num_leaves 31 --lgb_learning_rate 0.03 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_bagging_freq 5 --lgb_min_child_samples 100 --rf_n_estimators 300 --rf_max_depth 10

Expected: 93-95% AUC, 20-25 min, 10-12 GB RAM

1.5.2 WIN64 CONSERVATIVE TRAINING
Command:
python train_malware_models.py --category Win64 --data_dir ember2024_ultra_focused_features --output_dir conservative_win64 --sample_size 1000000 --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_n_estimators 1200 --lgb_num_leaves 31 --lgb_learning_rate 0.03 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_bagging_freq 5 --lgb_min_child_samples 100 --rf_n_estimators 250 --rf_max_depth 10

Expected: 92-94% AUC, 15-18 min, 8-10 GB RAM

1.5.3 APK CONSERVATIVE TRAINING (CORRECTED)
Command:
python train_malware_models.py --category APK --data_dir ember2024_ultra_focused_features --output_dir conservative_apk --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_n_estimators 1000 --lgb_num_leaves 31 --lgb_learning_rate 0.04 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_bagging_freq 5 --lgb_min_child_samples 80 --rf_n_estimators 200 --rf_max_depth 10

Expected: 90-93% AUC, 10-12 min, 6-8 GB RAM (Uses full 416K dataset)

1.5.4 NET CONSERVATIVE TRAINING
Command:
python train_malware_models.py --category Net --data_dir ember2024_ultra_focused_features --output_dir conservative_dotnet --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_n_estimators 1000 --lgb_num_leaves 31 --lgb_learning_rate 0.04 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_bagging_freq 5 --lgb_min_child_samples 80 --rf_n_estimators 200 --rf_max_depth 10

Expected: 91-94% AUC, 10-12 min, 6-8 GB RAM (Uses full 520K dataset)

1.5.5 PDF CONSERVATIVE TRAINING
Command:
python train_malware_models.py --category PDF --data_dir ember2024_ultra_focused_features --output_dir conservative_pdf --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_n_estimators 800 --lgb_num_leaves 15 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_bagging_freq 5 --lgb_min_child_samples 50 --rf_n_estimators 150 --rf_max_depth 8

Expected: 88-92% AUC, 6-8 min, 3-4 GB RAM (Uses full 104K dataset)

1.5.6 ELF CONSERVATIVE TRAINING
Command:
python train_malware_models.py --category ELF --data_dir ember2024_ultra_focused_features --output_dir conservative_elf --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_n_estimators 600 --lgb_num_leaves 15 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_bagging_freq 5 --lgb_min_child_samples 30 --rf_n_estimators 100 --rf_max_depth 8

Expected: 86-90% AUC, 4-6 min, 2-3 GB RAM (Uses full 52K dataset)

1.6 CATEGORY-SPECIFIC BEST XGBOOST TRAINING ⭐
----------------------------------------------

1.6.1 WIN32 BEST XGBOOST (MAXIMUM PERFORMANCE)
Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir best_xgboost_win32 --sample_size 1500000 --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 4000 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 96-98% AUC, 18-25 min, 12-16 GB RAM

1.6.2 WIN64 BEST XGBOOST
Command:
python train_malware_models.py --category Win64 --data_dir ember2024_merged_categories --output_dir best_xgboost_win64 --sample_size 1000000 --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 95-97% AUC, 15-20 min, 10-12 GB RAM

1.6.3 APK BEST XGBOOST
Command:
python train_malware_models.py --category APK --data_dir ember2024_merged_categories --output_dir best_xgboost_apk --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 93-96% AUC, 10-12 min, 8-10 GB RAM (Uses full 416K dataset)

1.6.4 NET BEST XGBOOST
Command:
python train_malware_models.py --category Net --data_dir ember2024_merged_categories --output_dir best_xgboost_dotnet --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 94-97% AUC, 10-12 min, 8-10 GB RAM (Uses full 520K dataset)

1.6.5 PDF BEST XGBOOST
Command:
python train_malware_models.py --category PDF --data_dir ember2024_merged_categories --output_dir best_xgboost_pdf --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3000 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 91-95% AUC, 6-8 min, 4-5 GB RAM (Uses full 104K dataset)

1.6.6 ELF BEST XGBOOST
Command:
python train_malware_models.py --category ELF --data_dir ember2024_merged_categories --output_dir best_xgboost_elf --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 2500 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 89-94% AUC, 4-6 min, 3-4 GB RAM (Uses full 52K dataset)

1.7 MAXIMUM PERFORMANCE ALL-MODELS TRAINING ⭐⭐⭐
--------------------------------------------------
Purpose: Ultimate performance with all three models (LightGBM, XGBoost, Random Forest)
Strategy: Optimized parameters for maximum accuracy while preventing overfitting
Models: All three models trained simultaneously for comprehensive comparison

1.7.1 WIN32 MAXIMUM PERFORMANCE (ALL MODELS)
Command:
python train_malware_models.py --category Win32 --data_dir ember2024_ultra_focused_features --output_dir ultimate_win32 --sample_size 1500000 --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4500 --lgb_num_leaves 63 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 25 --xgb_n_estimators 4000 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05 --rf_n_estimators 400 --rf_max_depth 12 --rf_min_samples_split 5 --rf_min_samples_leaf 3

Expected: 97-99% AUC, 25-30 min, 14-18 GB RAM

1.7.2 WIN64 MAXIMUM PERFORMANCE (ALL MODELS)
Command:
python train_malware_models.py --category Win64 --data_dir ember2024_ultra_focused_features --output_dir ultimate_win64 --sample_size 1000000 --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4000 --lgb_num_leaves 63 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 25 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05 --rf_n_estimators 350 --rf_max_depth 12 --rf_min_samples_split 5 --rf_min_samples_leaf 3

Expected: 96-98% AUC, 20-25 min, 12-14 GB RAM

1.7.3 APK MAXIMUM PERFORMANCE (ALL MODELS)
Command:
python train_malware_models.py --category APK --data_dir ember2024_ultra_focused_features --output_dir ultimate_apk --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4000 --lgb_num_leaves 63 --lgb_learning_rate 0.09 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 20 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05 --rf_n_estimators 300 --rf_max_depth 12 --rf_min_samples_split 5 --rf_min_samples_leaf 3

Expected: 94-97% AUC, 15-18 min, 8-10 GB RAM (Uses full 416K dataset)

1.7.4 NET MAXIMUM PERFORMANCE (ALL MODELS)
Command:
python train_malware_models.py --category Net --data_dir ember2024_ultra_focused_features --output_dir ultimate_dotnet --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4000 --lgb_num_leaves 63 --lgb_learning_rate 0.09 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 20 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05 --rf_n_estimators 300 --rf_max_depth 12 --rf_min_samples_split 5 --rf_min_samples_leaf 3

Expected: 95-98% AUC, 15-18 min, 8-10 GB RAM (Uses full 520K dataset)

1.7.5 PDF MAXIMUM PERFORMANCE (ALL MODELS)
Command:
python train_malware_models.py --category PDF --data_dir ember2024_ultra_focused_features --output_dir ultimate_pdf --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 3500 --lgb_num_leaves 31 --lgb_learning_rate 0.1 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 15 --xgb_n_estimators 3000 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05 --rf_n_estimators 250 --rf_max_depth 10 --rf_min_samples_split 5 --rf_min_samples_leaf 3

Expected: 92-96% AUC, 8-10 min, 4-5 GB RAM (Uses full 104K dataset)

1.7.6 ELF MAXIMUM PERFORMANCE (ALL MODELS)
Command:
python train_malware_models.py --category ELF --data_dir ember2024_ultra_focused_features --output_dir ultimate_elf --models lightgbm xgboost randomforest --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 2500 --lgb_num_leaves 31 --lgb_learning_rate 0.1 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 10 --xgb_n_estimators 2500 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05 --rf_n_estimators 200 --rf_max_depth 10 --rf_min_samples_split 3 --rf_min_samples_leaf 2

Expected: 90-95% AUC, 6-8 min, 3-4 GB RAM (Uses full 52K dataset)

1.8 INDIVIDUAL MODEL OPTIMIZATION ⭐⭐
------------------------------------
Purpose: Best single-model performance for each category and model type
Strategy: Model-specific parameter optimization for maximum accuracy

1.8.1 BEST LIGHTGBM INDIVIDUAL TRAINING

WIN32 BEST LIGHTGBM:
python train_malware_models.py --category Win32 --data_dir ember2024_ultra_focused_features --output_dir best_lgb_win32 --sample_size 1500000 --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 5000 --lgb_num_leaves 63 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 25

WIN64 BEST LIGHTGBM:
python train_malware_models.py --category Win64 --data_dir ember2024_ultra_focused_features --output_dir best_lgb_win64 --sample_size 1000000 --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4500 --lgb_num_leaves 63 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 25

APK BEST LIGHTGBM:
python train_malware_models.py --category APK --data_dir ember2024_ultra_focused_features --output_dir best_lgb_apk --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4000 --lgb_num_leaves 63 --lgb_learning_rate 0.09 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 20

NET BEST LIGHTGBM:
python train_malware_models.py --category Net --data_dir ember2024_ultra_focused_features --output_dir best_lgb_dotnet --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4000 --lgb_num_leaves 63 --lgb_learning_rate 0.09 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 20

PDF BEST LIGHTGBM:
python train_malware_models.py --category PDF --data_dir ember2024_ultra_focused_features --output_dir best_lgb_pdf --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 3500 --lgb_num_leaves 31 --lgb_learning_rate 0.1 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 15

ELF BEST LIGHTGBM:
python train_malware_models.py --category ELF --data_dir ember2024_ultra_focused_features --output_dir best_lgb_elf --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 2500 --lgb_num_leaves 31 --lgb_learning_rate 0.1 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 10

1.8.2 BEST XGBOOST INDIVIDUAL TRAINING

WIN32 BEST XGBOOST:
python train_malware_models.py --category Win32 --data_dir ember2024_ultra_focused_features --output_dir best_xgb_win32 --sample_size 1500000 --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 4000 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

WIN64 BEST XGBOOST:
python train_malware_models.py --category Win64 --data_dir ember2024_ultra_focused_features --output_dir best_xgb_win64 --sample_size 1000000 --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

APK BEST XGBOOST:
python train_malware_models.py --category APK --data_dir ember2024_ultra_focused_features --output_dir best_xgb_apk --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

NET BEST XGBOOST:
python train_malware_models.py --category Net --data_dir ember2024_ultra_focused_features --output_dir best_xgb_dotnet --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

PDF BEST XGBOOST:
python train_malware_models.py --category PDF --data_dir ember2024_ultra_focused_features --output_dir best_xgb_pdf --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3000 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

ELF BEST XGBOOST:
python train_malware_models.py --category ELF --data_dir ember2024_ultra_focused_features --output_dir best_xgb_elf --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 2500 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

1.8.3 BEST RANDOM FOREST INDIVIDUAL TRAINING

WIN32 BEST RANDOM FOREST:
python train_malware_models.py --category Win32 --data_dir ember2024_ultra_focused_features --output_dir best_rf_win32 --sample_size 1500000 --models randomforest --cv_folds 7 --val_size 0.15 --random_state 42 --rf_n_estimators 500 --rf_max_depth 15 --rf_min_samples_split 3 --rf_min_samples_leaf 2 --rf_max_features sqrt --rf_bootstrap True --rf_oob_score True

WIN64 BEST RANDOM FOREST:
python train_malware_models.py --category Win64 --data_dir ember2024_ultra_focused_features --output_dir best_rf_win64 --sample_size 1000000 --models randomforest --cv_folds 7 --val_size 0.15 --random_state 42 --rf_n_estimators 400 --rf_max_depth 15 --rf_min_samples_split 3 --rf_min_samples_leaf 2 --rf_max_features sqrt --rf_bootstrap True --rf_oob_score True

APK BEST RANDOM FOREST:
python train_malware_models.py --category APK --data_dir ember2024_ultra_focused_features --output_dir best_rf_apk --models randomforest --cv_folds 7 --val_size 0.15 --random_state 42 --rf_n_estimators 350 --rf_max_depth 15 --rf_min_samples_split 3 --rf_min_samples_leaf 2 --rf_max_features sqrt --rf_bootstrap True --rf_oob_score True

NET BEST RANDOM FOREST:
python train_malware_models.py --category Net --data_dir ember2024_ultra_focused_features --output_dir best_rf_dotnet --models randomforest --cv_folds 7 --val_size 0.15 --random_state 42 --rf_n_estimators 350 --rf_max_depth 15 --rf_min_samples_split 3 --rf_min_samples_leaf 2 --rf_max_features sqrt --rf_bootstrap True --rf_oob_score True

PDF BEST RANDOM FOREST:
python train_malware_models.py --category PDF --data_dir ember2024_ultra_focused_features --output_dir best_rf_pdf --models randomforest --cv_folds 7 --val_size 0.15 --random_state 42 --rf_n_estimators 300 --rf_max_depth 12 --rf_min_samples_split 3 --rf_min_samples_leaf 2 --rf_max_features sqrt --rf_bootstrap True --rf_oob_score True

ELF BEST RANDOM FOREST:
python train_malware_models.py --category ELF --data_dir ember2024_ultra_focused_features --output_dir best_rf_elf --models randomforest --cv_folds 7 --val_size 0.15 --random_state 42 --rf_n_estimators 250 --rf_max_depth 12 --rf_min_samples_split 2 --rf_min_samples_leaf 1 --rf_max_features sqrt --rf_bootstrap True --rf_oob_score True

===============================================================================
2. PRODUCTION-READY TRAINING COMMANDS
===============================================================================

2.1 CONSERVATIVE TRAINING (MAXIMUM ANTI-OVERFITTING)
----------------------------------------------------
Purpose: Minimize overfitting for deployment in production
Trade-off: Slightly lower training performance for better generalization

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir conservative_models --sample_size 1000000 --models lightgbm xgboost --cv_folds 10 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_learning_rate 0.03 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_min_child_samples 100 --xgb_learning_rate 0.03 --xgb_max_depth 4 --xgb_subsample 0.7 --xgb_colsample_bytree 0.7 --xgb_reg_alpha 0.2 --xgb_reg_lambda 2.0

Key Differences:
- Lower learning rates (0.03 vs 0.05)
- More aggressive regularization
- 10-fold CV for robust evaluation
- Deeper tree constraints

2.2 FAST TRAINING (DEVELOPMENT/TESTING)
---------------------------------------
Purpose: Quick model training for development and testing
Trade-off: Higher learning rate, smaller dataset

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir fast_models --sample_size 100000 --models lightgbm xgboost --cv_folds 3 --val_size 0.2 --early_stopping_rounds 30 --random_state 42 --lgb_learning_rate 0.1 --lgb_n_estimators 500 --xgb_learning_rate 0.1 --xgb_n_estimators 500

Expected Training Time: 2-3 minutes
Use Case: Rapid prototyping, parameter tuning

2.3 MEMORY-EFFICIENT TRAINING
-----------------------------
Purpose: Training on systems with limited memory
Sample Size: 500,000 samples

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir memory_efficient_models --sample_size 500000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.8 --lgb_bagging_fraction 0.8 --xgb_learning_rate 0.05 --xgb_subsample 0.8 --xgb_colsample_bytree 0.8

Expected Memory Usage: <8GB RAM
Suitable for: Standard desktop/laptop systems

===============================================================================
3. CATEGORY-SPECIFIC TRAINING COMMANDS
===============================================================================

3.1 WINDOWS PE FILES (WIN32 + WIN64)
------------------------------------
Purpose: Specialized training for Windows executables
Coverage: Both 32-bit and 64-bit Windows PE files

Command:
python train_malware_models.py --category Win32 Win64 --data_dir ember2024_merged_categories --output_dir windows_pe_models --sample_size 1200000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.8 --lgb_bagging_fraction 0.8 --xgb_learning_rate 0.05 --xgb_max_depth 5 --xgb_subsample 0.8 --xgb_colsample_bytree 0.8

Sample Distribution: ~75% Win32, ~25% Win64 (based on dataset sizes)

3.2 MOBILE MALWARE (APK)
------------------------
Purpose: Android malware detection specialist
Dataset: APK files only

Command:
python train_malware_models.py --category APK --data_dir ember2024_merged_categories --output_dir android_models --sample_size 400000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.05 --xgb_max_depth 6 --xgb_subsample 0.9 --xgb_colsample_bytree 0.9

Note: Uses full APK dataset (416K samples available)

3.3 DOCUMENT MALWARE (PDF)
--------------------------
Purpose: PDF malware detection
Dataset: PDF files only

Command:
python train_malware_models.py --category PDF --data_dir ember2024_merged_categories --output_dir pdf_models --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.08 --xgb_max_depth 6 --xgb_subsample 0.9 --xgb_colsample_bytree 0.9

Note: Uses full PDF dataset (104K samples) - no sampling needed

3.4 UNIX/LINUX FILES (ELF)
--------------------------
Purpose: Linux/Unix executable malware detection
Dataset: ELF files only

Command:
python train_malware_models.py --category ELF --data_dir ember2024_merged_categories --output_dir linux_models --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.08 --xgb_max_depth 6 --xgb_subsample 0.9

Note: Uses full ELF dataset (52K samples) - no sampling needed

3.5 .NET ASSEMBLIES
-------------------
Purpose: .NET malware detection
Dataset: .NET files only

Command:
python train_malware_models.py --category Net --data_dir ember2024_merged_categories --output_dir dotnet_models --sample_size 500000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.05 --xgb_max_depth 6 --xgb_subsample 0.9 --xgb_colsample_bytree 0.9

Sample Distribution: ~96% of available .NET samples (520K total)

===============================================================================
4. PARAMETER OPTIMIZATION GUIDE
===============================================================================

4.1 ANTI-OVERFITTING PARAMETERS (UPDATED)
-----------------------------------------

LightGBM Anti-Overfitting:
- learning_rate: 0.02-0.04 (ultra-low for stability)
- feature_fraction: 0.6-0.7 (aggressive feature subsampling)
- bagging_fraction: 0.6-0.7 (aggressive data subsampling)
- min_child_samples: 80-150 (prevent small leaves)
- num_leaves: 15-31 (control tree complexity)
- early_stopping_rounds: 20-30 (stop early when no improvement)
- bagging_freq: 5-7 (frequent bagging for variance reduction)

XGBoost Anti-Overfitting:
- learning_rate: 0.02-0.04 (ultra-low for stability)
- max_depth: 3-4 (severely limit tree depth)
- subsample: 0.6-0.7 (aggressive sample subsampling)
- colsample_bytree: 0.6-0.7 (aggressive feature subsampling)
- min_child_weight: 5-10 (prevent small leaves)
- reg_alpha: 0.2-0.5 (strong L1 regularization)
- reg_lambda: 2.0-5.0 (strong L2 regularization)

Random Forest Anti-Overfitting:
- n_estimators: 100-300 (moderate ensemble size)
- max_depth: 8-12 (limit tree depth)
- min_samples_split: 10-20 (require more samples to split)
- min_samples_leaf: 5-10 (require more samples in leaves)
- max_features: 'sqrt' or 'log2' (feature subsampling)
- bootstrap: True (enable bootstrapping)
- oob_score: True (out-of-bag validation)

4.2 PERFORMANCE OPTIMIZATION
----------------------------

For Higher Performance (if overfitting is not an issue):
- Increase learning_rate to 0.1
- Increase feature/bagging fractions to 0.9
- Increase max_depth to 6-8
- Increase n_estimators to 3000+
- Reduce regularization (lower reg_alpha/lambda)

For Memory Optimization:
- Reduce sample_size
- Use feature_fraction < 0.8
- Limit n_estimators to 1000
- Use single model instead of both

4.3 MAXIMUM PERFORMANCE PARAMETERS ⭐
-------------------------------------

BEST LIGHTGBM PARAMETERS (Category-Optimized):
Supported Parameters Only:
- n_estimators: 4500-5000 (full convergence)
- num_leaves: 63 (complex trees) for large datasets, 31 for small datasets
- learning_rate: 0.08-0.1 (optimal balance)
- feature_fraction: 0.95 (use almost all features)
- bagging_fraction: 0.95 (use almost all data)
- bagging_freq: 3 (frequent bagging)
- min_child_samples: 10-25 (allow smaller leaves based on dataset size)

Large Datasets (Win32, Win64):
- n_estimators: 4500-5000
- num_leaves: 63
- learning_rate: 0.08
- min_child_samples: 25

Medium Datasets (APK, .NET):
- n_estimators: 4000
- num_leaves: 63
- learning_rate: 0.09
- min_child_samples: 20

Small Datasets (PDF, ELF):
- n_estimators: 2500-3500
- num_leaves: 31 (reduced complexity)
- learning_rate: 0.1
- min_child_samples: 10-15

BEST XGBOOST PARAMETERS (Category-Optimized):
Supported Parameters Only:
- n_estimators: 3500-4000 (full convergence)
- max_depth: 6-7 (deep trees)
- learning_rate: 0.08-0.1 (optimal balance)
- subsample: 0.95 (use almost all samples)
- colsample_bytree: 0.95 (use almost all features)
- min_child_weight: 1 (allow small leaves)
- reg_alpha: 0.01 (minimal L1 regularization)
- reg_lambda: 0.05 (minimal L2 regularization)

Large Datasets (Win32, Win64):
- n_estimators: 3500-4000
- max_depth: 7
- learning_rate: 0.08

Medium Datasets (APK, .NET):
- n_estimators: 3500
- max_depth: 7
- learning_rate: 0.09

Small Datasets (PDF, ELF):
- n_estimators: 2500-3000
- max_depth: 6
- learning_rate: 0.1

Note: Only parameters supported by the training script are listed above.

===============================================================================
5. EVALUATION AND CROSS-VALIDATION
===============================================================================

5.1 CROSS-VALIDATION SETTINGS
-----------------------------

Standard CV: --cv_folds 5
- Good balance of computational cost and reliability
- Suitable for most training scenarios

Robust CV: --cv_folds 10
- More reliable results but longer training time
- Recommended for final model selection

Quick CV: --cv_folds 3
- Faster training for development
- Less reliable but good for rapid iteration

5.2 VALIDATION SPLIT OPTIONS
----------------------------

Standard: --val_size 0.2 (20% validation)
Conservative: --val_size 0.25 (25% validation)
Aggressive: --val_size 0.15 (15% validation)

===============================================================================
6. OUTPUT AND MODEL MANAGEMENT
===============================================================================

6.1 OUTPUT DIRECTORY STRUCTURE
------------------------------
models/
├── lightgbm_[category]_model.txt     # LightGBM model file
├── xgboost_[category]_model.json     # XGBoost model file
├── training_results_[timestamp].json  # Detailed metrics
└── visualizations/                    # Performance plots
    ├── roc_curve_[model]_[category].png
    ├── pr_curve_[model]_[category].png
    ├── confusion_matrix_[model]_[category].png
    ├── feature_importance_[model]_[category].png
    └── model_performance_comparison.png

6.2 RECOMMENDED OUTPUT DIRECTORIES
----------------------------------
- optimal_models/          # Production-ready models
- conservative_models/     # Maximum anti-overfitting
- multi_category_models/   # Multiple file types
- universal_models/        # All categories
- fast_models/            # Quick development models
- category_specific/      # Single category models

===============================================================================
7. TROUBLESHOOTING COMMON ISSUES
===============================================================================

7.1 MEMORY ERRORS
-----------------
Problem: "MemoryError" or system freezing
Solution: Reduce --sample_size parameter
Example: Use 500000 instead of 1000000

7.2 SLOW TRAINING
-----------------
Problem: Training takes too long
Solutions:
- Increase learning rates (0.1 instead of 0.05)
- Reduce n_estimators (1000 instead of 2000)
- Reduce CV folds (3 instead of 5)
- Use smaller sample size

7.3 OVERFITTING DETECTION
-------------------------
Signs of Overfitting:
- Large gap between validation and test AUC (>0.05)
- Perfect validation scores (1.0000)
- Poor cross-validation consistency

Solutions:
- Use conservative parameters (section 2.1)
- Increase regularization
- Reduce model complexity
- Add more training data

7.4 POOR PERFORMANCE
-------------------
Problem: Low AUC scores (<0.90)
Potential Causes:
- Insufficient training data
- Too much regularization
- Learning rate too low
- Early stopping too aggressive

Solutions:
- Increase sample size
- Reduce regularization parameters
- Increase learning rate to 0.08-0.1
- Increase early_stopping_rounds to 100

===============================================================================
8. RECOMMENDED TRAINING WORKFLOW
===============================================================================

8.1 DEVELOPMENT PHASE
---------------------
1. Start with fast training (section 2.2)
2. Experiment with different categories
3. Tune key parameters (learning_rate, regularization)
4. Use 3-fold CV for quick feedback

8.2 VALIDATION PHASE
--------------------
1. Use optimal training (section 1.1)
2. Run 5-fold cross-validation
3. Compare multiple models
4. Validate on hold-out test set

8.3 PRODUCTION PHASE
--------------------
1. Use conservative training (section 2.1)
2. Run 10-fold cross-validation
3. Train on maximum available data
4. Thoroughly validate generalization

===============================================================================
9. PERFORMANCE BENCHMARKS
===============================================================================

Expected Performance Ranges:

ULTIMATE ALL-MODELS TRAINING ⭐⭐⭐ (Section 1.7):
Category         | Expected AUC | Training Time | Memory Usage
---------------- | ------------ | ------------- | ------------
Win32 Ultimate   | 0.97-0.99    | 25-30 min     | 14-18 GB
Win64 Ultimate   | 0.96-0.98    | 20-25 min     | 12-14 GB
APK Ultimate     | 0.94-0.97    | 15-18 min     | 8-10 GB
PDF Ultimate     | 0.92-0.96    | 8-10 min      | 4-5 GB
ELF Ultimate     | 0.90-0.95    | 6-8 min       | 3-4 GB
.NET Ultimate    | 0.95-0.98    | 15-18 min     | 8-10 GB

INDIVIDUAL MODEL OPTIMIZATION ⭐⭐ (Section 1.8):
Category         | LightGBM AUC | XGBoost AUC  | Random Forest AUC
---------------- | ------------ | ------------ | -----------------
Win32            | 0.98-0.99    | 0.97-0.98    | 0.95-0.97
Win64            | 0.97-0.98    | 0.96-0.97    | 0.94-0.96
APK              | 0.95-0.97    | 0.94-0.96    | 0.92-0.95
PDF              | 0.93-0.96    | 0.92-0.95    | 0.90-0.93
ELF              | 0.91-0.95    | 0.90-0.94    | 0.88-0.92
.NET             | 0.96-0.98    | 0.95-0.97    | 0.93-0.96

STANDARD TRAINING:
Category         | Expected AUC | Training Time | Memory Usage
---------------- | ------------ | ------------- | ------------
Win32            | 0.94-0.97    | 10-15 min     | 8-12 GB
Win64            | 0.92-0.96    | 6-10 min      | 6-8 GB
APK              | 0.90-0.95    | 4-6 min       | 4-6 GB
PDF              | 0.88-0.94    | 2-3 min       | 2-3 GB
ELF              | 0.85-0.93    | 1-2 min       | 1-2 GB
.NET             | 0.91-0.96    | 4-6 min       | 4-6 GB
All Combined     | 0.93-0.96    | 20-30 min     | 12-16 GB

BEST LIGHTGBM TRAINING ⭐:
Category         | Expected AUC | Training Time | Memory Usage
---------------- | ------------ | ------------- | ------------
Win32 Best LGB   | 0.97-0.99    | 15-20 min     | 10-14 GB
Win64 Best LGB   | 0.96-0.98    | 12-15 min     | 8-10 GB
APK Best LGB     | 0.94-0.97    | 8-10 min      | 6-8 GB
PDF Best LGB     | 0.92-0.96    | 4-6 min       | 3-4 GB
ELF Best LGB     | 0.90-0.95    | 3-4 min       | 2-3 GB
.NET Best LGB    | 0.95-0.98    | 8-10 min      | 6-8 GB

BEST XGBOOST TRAINING ⭐:
Category         | Expected AUC | Training Time | Memory Usage
---------------- | ------------ | ------------- | ------------
Win32 Best XGB   | 0.96-0.98    | 18-25 min     | 12-16 GB
Win64 Best XGB   | 0.95-0.97    | 15-20 min     | 10-12 GB
APK Best XGB     | 0.93-0.96    | 10-12 min     | 8-10 GB
PDF Best XGB     | 0.91-0.95    | 6-8 min       | 4-5 GB
ELF Best XGB     | 0.89-0.94    | 4-6 min       | 3-4 GB
.NET Best XGB    | 0.94-0.97    | 10-12 min     | 8-10 GB

Note: Performance varies based on system specifications and parameters used.
⭐ Best configurations optimized for maximum accuracy with proper regularization.
⭐⭐ Individual model optimization for specialized use cases.  
⭐⭐⭐ Ultimate performance with all three models for comprehensive comparison.

===============================================================================
10. QUICK REFERENCE SUMMARY
===============================================================================

RECOMMENDED TRAINING HIERARCHY:
1. Ultimate All-Models Training (Section 1.7) - Best overall performance
2. Individual Model Optimization (Section 1.8) - Specialized single-model training
3. Conservative Anti-Overfitting (Section 1.5) - Production deployment
4. Production-Ready Training (Section 1.1-1.4) - Balanced performance/stability

KEY FEATURES ADDED:
✅ All three models training simultaneously
✅ Individual model optimization for each category
✅ Maximum performance configurations
✅ Overfitting prevention strategies
✅ Comprehensive parameter tuning
✅ Memory and time optimization
✅ Performance benchmarks for all scenarios

TOTAL COMMANDS PROVIDED: 50+ optimized training scenarios
CATEGORIES COVERED: Win32, Win64, APK, PDF, ELF, .NET
MODELS SUPPORTED: LightGBM, XGBoost, Random Forest
ANTI-OVERFITTING: Conservative parameters and robust validation

===============================================================================
11. CONTACT AND SUPPORT
===============================================================================

For issues with training commands:
1. Check parameter syntax using: python train_malware_models.py --help
2. Verify dataset paths and file existence
3. Monitor system resources during training
4. Review training logs for error messages

Training Script Location: train_malware_models.py
Dataset Location: ember2024_merged_categories/
Documentation: DATASET_STATUS_REPORT.md

===============================================================================
END OF GUIDE
===============================================================================

Last Updated: September 5, 2025
Version: 2.0 - Enhanced with All-Models Training
Total Commands Provided: 50+ comprehensive training scenarios
Compatible with: EMBER2024 Ultra-Focused Feature Dataset
New Features: Ultimate all-models training, individual optimization, enhanced anti-overfitting
