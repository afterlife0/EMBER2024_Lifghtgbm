EMBER2024 Malware Detection Training Commands Guide
==================================================
Generated: August 28, 2025
Author: EMBER2024 Training Pipeline
Version: 1.0

OVERVIEW
========
This guide provides optimal training commands for different scenarios using the EMBER2024 
malware detection dataset. All commands are designed to prevent overfitting while maintaining
high performance across different file types and use cases.

Dataset Information:
- Win32: 3,120,000 train + 720,000 test samples
- Win64: 1,040,000 train + 240,000 test samples  
- .NET: 520,000 train + 120,000 test samples
- APK: 416,000 train + 96,000 test samples
- PDF: 104,000 train + 24,000 test samples
- ELF: 52,000 train + 12,000 test samples

===============================================================================
1. OPTIMAL TRAINING COMMANDS
===============================================================================

1.1 PRODUCTION-READY TRAINING (RECOMMENDED)
-------------------------------------------
Purpose: Best balance of performance and generalization
Dataset: Win32 (largest, most diverse)
Sample Size: 1,000,000 samples
Models: Both LightGBM and XGBoost

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir optimal_models --sample_size 1000000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_n_estimators 2000 --lgb_num_leaves 31 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.8 --lgb_bagging_fraction 0.8 --lgb_bagging_freq 5 --lgb_min_child_samples 50 --xgb_n_estimators 2000 --xgb_max_depth 5 --xgb_learning_rate 0.05 --xgb_subsample 0.8 --xgb_colsample_bytree 0.8 --xgb_min_child_weight 3 --xgb_reg_alpha 0.1 --xgb_reg_lambda 1.0

Expected Training Time: 10-15 minutes
Expected Performance: >95% AUC with good generalization

1.2 MULTI-CATEGORY COMPREHENSIVE TRAINING
-----------------------------------------
Purpose: Train on multiple file types for broader coverage
Dataset: Win32 + Win64 + APK (covers Windows and Android)
Sample Size: 1,500,000 samples (distributed across categories)

Command:
python train_malware_models.py --category Win32 Win64 APK --data_dir ember2024_merged_categories --output_dir multi_category_models --sample_size 1500000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.8 --lgb_bagging_fraction 0.8 --lgb_min_child_samples 50 --xgb_learning_rate 0.05 --xgb_max_depth 5 --xgb_subsample 0.8 --xgb_colsample_bytree 0.8 --xgb_reg_alpha 0.1 --xgb_reg_lambda 1.0

Expected Training Time: 15-20 minutes
Coverage: Windows PE files and Android APK files

1.3 ALL CATEGORIES TRAINING
---------------------------
Purpose: Maximum coverage across all file types
Dataset: All 6 categories (Win32, Win64, APK, PDF, ELF, .NET)
Sample Size: 2,000,000 samples (smart distributed)

Command:
python train_malware_models.py --category all --data_dir ember2024_merged_categories --output_dir universal_models --sample_size 2000000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.8 --lgb_bagging_fraction 0.8 --lgb_min_child_samples 50 --xgb_learning_rate 0.05 --xgb_max_depth 5 --xgb_subsample 0.8 --xgb_colsample_bytree 0.8 --xgb_reg_alpha 0.1 --xgb_reg_lambda 1.0

Expected Training Time: 20-30 minutes
Coverage: Universal malware detector for all file types

1.4 LIGHTGBM BEST PERFORMANCE TRAINING ⭐ (MAXIMUM RESULTS)
----------------------------------------------------------
Purpose: Achieve absolute best LightGBM performance with optimized parameters
Dataset: Win32 (largest dataset for maximum learning)
Sample Size: 1,500,000 samples
Model: LightGBM only (optimized for best results)

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir best_lightgbm_models --sample_size 1500000 --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 5000 --lgb_num_leaves 63 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 25

Performance Optimizations:
- Higher num_leaves (63 vs 31): More complex trees for better fitting
- Optimized learning_rate (0.08): Balance between speed and accuracy
- High feature/bagging fractions (0.95): Use most data for best results
- Extended estimators (5000): Allow model to fully converge
- Larger validation set: More reliable performance estimation
- 7-fold CV: Robust evaluation without excessive computation
- Extended early stopping (100): Prevent premature stopping

Expected Training Time: 15-20 minutes
Expected Performance: >97% AUC with excellent generalization
Memory Usage: 10-14 GB
Best for: Production deployment requiring maximum accuracy

1.5 CATEGORY-SPECIFIC BEST LIGHTGBM TRAINING ⭐
-----------------------------------------------

1.5.1 WIN32 BEST LIGHTGBM (MAXIMUM PERFORMANCE)
Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir best_lightgbm_win32 --sample_size 1500000 --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 5000 --lgb_num_leaves 63 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 25

Expected: 97-99% AUC, 15-20 min, 10-14 GB RAM

1.5.2 WIN64 BEST LIGHTGBM
Command:
python train_malware_models.py --category Win64 --data_dir ember2024_merged_categories --output_dir best_lightgbm_win64 --sample_size 1000000 --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4500 --lgb_num_leaves 63 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 25

Expected: 96-98% AUC, 12-15 min, 8-10 GB RAM

1.5.3 APK BEST LIGHTGBM
Command:
python train_malware_models.py --category APK --data_dir ember2024_merged_categories --output_dir best_lightgbm_apk --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4000 --lgb_num_leaves 63 --lgb_learning_rate 0.09 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 20

Expected: 94-97% AUC, 8-10 min, 6-8 GB RAM (Uses full 416K dataset)

1.5.4 NET BEST LIGHTGBM
Command:
python train_malware_models.py --category Net --data_dir ember2024_merged_categories --output_dir best_lightgbm_dotnet --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 4000 --lgb_num_leaves 63 --lgb_learning_rate 0.09 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 20

Expected: 95-98% AUC, 8-10 min, 6-8 GB RAM (Uses full 520K dataset)

1.5.5 PDF BEST LIGHTGBM
Command:
python train_malware_models.py --category PDF --data_dir ember2024_merged_categories --output_dir best_lightgbm_pdf --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 3500 --lgb_num_leaves 31 --lgb_learning_rate 0.1 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 15

Expected: 92-96% AUC, 4-6 min, 3-4 GB RAM (Uses full 104K dataset)

1.5.6 ELF BEST LIGHTGBM
Command:
python train_malware_models.py --category ELF --data_dir ember2024_merged_categories --output_dir best_lightgbm_elf --models lightgbm --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --lgb_n_estimators 3000 --lgb_num_leaves 31 --lgb_learning_rate 0.1 --lgb_feature_fraction 0.95 --lgb_bagging_fraction 0.95 --lgb_bagging_freq 3 --lgb_min_child_samples 10

Expected: 90-95% AUC, 3-4 min, 2-3 GB RAM (Uses full 52K dataset)

1.6 CATEGORY-SPECIFIC BEST XGBOOST TRAINING ⭐
----------------------------------------------

1.6.1 WIN32 BEST XGBOOST (MAXIMUM PERFORMANCE)
Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir best_xgboost_win32 --sample_size 1500000 --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 4000 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 96-98% AUC, 18-25 min, 12-16 GB RAM

1.6.2 WIN64 BEST XGBOOST
Command:
python train_malware_models.py --category Win64 --data_dir ember2024_merged_categories --output_dir best_xgboost_win64 --sample_size 1000000 --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.08 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 95-97% AUC, 15-20 min, 10-12 GB RAM

1.6.3 APK BEST XGBOOST
Command:
python train_malware_models.py --category APK --data_dir ember2024_merged_categories --output_dir best_xgboost_apk --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 93-96% AUC, 10-12 min, 8-10 GB RAM (Uses full 416K dataset)

1.6.4 NET BEST XGBOOST
Command:
python train_malware_models.py --category Net --data_dir ember2024_merged_categories --output_dir best_xgboost_dotnet --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3500 --xgb_max_depth 7 --xgb_learning_rate 0.09 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 94-97% AUC, 10-12 min, 8-10 GB RAM (Uses full 520K dataset)

1.6.5 PDF BEST XGBOOST
Command:
python train_malware_models.py --category PDF --data_dir ember2024_merged_categories --output_dir best_xgboost_pdf --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 3000 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 91-95% AUC, 6-8 min, 4-5 GB RAM (Uses full 104K dataset)

1.6.6 ELF BEST XGBOOST
Command:
python train_malware_models.py --category ELF --data_dir ember2024_merged_categories --output_dir best_xgboost_elf --models xgboost --cv_folds 7 --val_size 0.15 --early_stopping_rounds 100 --random_state 42 --xgb_n_estimators 2500 --xgb_max_depth 6 --xgb_learning_rate 0.1 --xgb_subsample 0.95 --xgb_colsample_bytree 0.95 --xgb_min_child_weight 1 --xgb_reg_alpha 0.01 --xgb_reg_lambda 0.05

Expected: 89-94% AUC, 4-6 min, 3-4 GB RAM (Uses full 52K dataset)

===============================================================================
2. SPECIALIZED TRAINING COMMANDS
===============================================================================

2.1 CONSERVATIVE TRAINING (MAXIMUM ANTI-OVERFITTING)
----------------------------------------------------
Purpose: Minimize overfitting for deployment in production
Trade-off: Slightly lower training performance for better generalization

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir conservative_models --sample_size 1000000 --models lightgbm xgboost --cv_folds 10 --val_size 0.25 --early_stopping_rounds 30 --random_state 42 --lgb_learning_rate 0.03 --lgb_feature_fraction 0.7 --lgb_bagging_fraction 0.7 --lgb_min_child_samples 100 --xgb_learning_rate 0.03 --xgb_max_depth 4 --xgb_subsample 0.7 --xgb_colsample_bytree 0.7 --xgb_reg_alpha 0.2 --xgb_reg_lambda 2.0

Key Differences:
- Lower learning rates (0.03 vs 0.05)
- More aggressive regularization
- 10-fold CV for robust evaluation
- Deeper tree constraints

2.2 FAST TRAINING (DEVELOPMENT/TESTING)
---------------------------------------
Purpose: Quick model training for development and testing
Trade-off: Higher learning rate, smaller dataset

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir fast_models --sample_size 100000 --models lightgbm xgboost --cv_folds 3 --val_size 0.2 --early_stopping_rounds 30 --random_state 42 --lgb_learning_rate 0.1 --lgb_n_estimators 500 --xgb_learning_rate 0.1 --xgb_n_estimators 500

Expected Training Time: 2-3 minutes
Use Case: Rapid prototyping, parameter tuning

2.3 MEMORY-EFFICIENT TRAINING
-----------------------------
Purpose: Training on systems with limited memory
Sample Size: 500,000 samples

Command:
python train_malware_models.py --category Win32 --data_dir ember2024_merged_categories --output_dir memory_efficient_models --sample_size 500000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.8 --lgb_bagging_fraction 0.8 --xgb_learning_rate 0.05 --xgb_subsample 0.8 --xgb_colsample_bytree 0.8

Expected Memory Usage: <8GB RAM
Suitable for: Standard desktop/laptop systems

===============================================================================
3. CATEGORY-SPECIFIC TRAINING COMMANDS
===============================================================================

3.1 WINDOWS PE FILES (WIN32 + WIN64)
------------------------------------
Purpose: Specialized training for Windows executables
Coverage: Both 32-bit and 64-bit Windows PE files

Command:
python train_malware_models.py --category Win32 Win64 --data_dir ember2024_merged_categories --output_dir windows_pe_models --sample_size 1200000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.8 --lgb_bagging_fraction 0.8 --xgb_learning_rate 0.05 --xgb_max_depth 5 --xgb_subsample 0.8 --xgb_colsample_bytree 0.8

Sample Distribution: ~75% Win32, ~25% Win64 (based on dataset sizes)

3.2 MOBILE MALWARE (APK)
------------------------
Purpose: Android malware detection specialist
Dataset: APK files only

Command:
python train_malware_models.py --category APK --data_dir ember2024_merged_categories --output_dir android_models --sample_size 400000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.05 --xgb_max_depth 6 --xgb_subsample 0.9 --xgb_colsample_bytree 0.9

Note: Uses full APK dataset (416K samples available)

3.3 DOCUMENT MALWARE (PDF)
--------------------------
Purpose: PDF malware detection
Dataset: PDF files only

Command:
python train_malware_models.py --category PDF --data_dir ember2024_merged_categories --output_dir pdf_models --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.08 --xgb_max_depth 6 --xgb_subsample 0.9 --xgb_colsample_bytree 0.9

Note: Uses full PDF dataset (104K samples) - no sampling needed

3.4 UNIX/LINUX FILES (ELF)
--------------------------
Purpose: Linux/Unix executable malware detection
Dataset: ELF files only

Command:
python train_malware_models.py --category ELF --data_dir ember2024_merged_categories --output_dir linux_models --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.08 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.08 --xgb_max_depth 6 --xgb_subsample 0.9

Note: Uses full ELF dataset (52K samples) - no sampling needed

3.5 .NET ASSEMBLIES
-------------------
Purpose: .NET malware detection
Dataset: .NET files only

Command:
python train_malware_models.py --category Net --data_dir ember2024_merged_categories --output_dir dotnet_models --sample_size 500000 --models lightgbm xgboost --cv_folds 5 --val_size 0.2 --early_stopping_rounds 50 --random_state 42 --lgb_learning_rate 0.05 --lgb_feature_fraction 0.9 --lgb_bagging_fraction 0.9 --xgb_learning_rate 0.05 --xgb_max_depth 6 --xgb_subsample 0.9 --xgb_colsample_bytree 0.9

Sample Distribution: ~96% of available .NET samples (520K total)

===============================================================================
4. PARAMETER OPTIMIZATION GUIDE
===============================================================================

4.1 ANTI-OVERFITTING PARAMETERS
-------------------------------

LightGBM Anti-Overfitting:
- learning_rate: 0.03-0.05 (lower = less overfitting)
- feature_fraction: 0.7-0.8 (use subset of features)
- bagging_fraction: 0.7-0.8 (use subset of data)
- min_child_samples: 50-100 (prevent small leaves)
- num_leaves: 31 or less (control tree complexity)
- early_stopping_rounds: 30-50 (stop when no improvement)

XGBoost Anti-Overfitting:
- learning_rate: 0.03-0.05 (lower = less overfitting)
- max_depth: 4-5 (limit tree depth)
- subsample: 0.7-0.8 (use subset of samples)
- colsample_bytree: 0.7-0.8 (use subset of features)
- min_child_weight: 3-5 (prevent small leaves)
- reg_alpha: 0.1-0.2 (L1 regularization)
- reg_lambda: 1.0-2.0 (L2 regularization)

4.2 PERFORMANCE OPTIMIZATION
----------------------------

For Higher Performance (if overfitting is not an issue):
- Increase learning_rate to 0.1
- Increase feature/bagging fractions to 0.9
- Increase max_depth to 6-8
- Increase n_estimators to 3000+
- Reduce regularization (lower reg_alpha/lambda)

For Memory Optimization:
- Reduce sample_size
- Use feature_fraction < 0.8
- Limit n_estimators to 1000
- Use single model instead of both

4.3 MAXIMUM PERFORMANCE PARAMETERS ⭐
-------------------------------------

BEST LIGHTGBM PARAMETERS (Category-Optimized):
Supported Parameters Only:
- n_estimators: 4500-5000 (full convergence)
- num_leaves: 63 (complex trees) for large datasets, 31 for small datasets
- learning_rate: 0.08-0.1 (optimal balance)
- feature_fraction: 0.95 (use almost all features)
- bagging_fraction: 0.95 (use almost all data)
- bagging_freq: 3 (frequent bagging)
- min_child_samples: 10-25 (allow smaller leaves based on dataset size)

Large Datasets (Win32, Win64):
- n_estimators: 4500-5000
- num_leaves: 63
- learning_rate: 0.08
- min_child_samples: 25

Medium Datasets (APK, .NET):
- n_estimators: 4000
- num_leaves: 63
- learning_rate: 0.09
- min_child_samples: 20

Small Datasets (PDF, ELF):
- n_estimators: 2500-3500
- num_leaves: 31 (reduced complexity)
- learning_rate: 0.1
- min_child_samples: 10-15

BEST XGBOOST PARAMETERS (Category-Optimized):
Supported Parameters Only:
- n_estimators: 3500-4000 (full convergence)
- max_depth: 6-7 (deep trees)
- learning_rate: 0.08-0.1 (optimal balance)
- subsample: 0.95 (use almost all samples)
- colsample_bytree: 0.95 (use almost all features)
- min_child_weight: 1 (allow small leaves)
- reg_alpha: 0.01 (minimal L1 regularization)
- reg_lambda: 0.05 (minimal L2 regularization)

Large Datasets (Win32, Win64):
- n_estimators: 3500-4000
- max_depth: 7
- learning_rate: 0.08

Medium Datasets (APK, .NET):
- n_estimators: 3500
- max_depth: 7
- learning_rate: 0.09

Small Datasets (PDF, ELF):
- n_estimators: 2500-3000
- max_depth: 6
- learning_rate: 0.1

Note: Only parameters supported by the training script are listed above.

===============================================================================
5. EVALUATION AND CROSS-VALIDATION
===============================================================================

5.1 CROSS-VALIDATION SETTINGS
-----------------------------

Standard CV: --cv_folds 5
- Good balance of computational cost and reliability
- Suitable for most training scenarios

Robust CV: --cv_folds 10
- More reliable results but longer training time
- Recommended for final model selection

Quick CV: --cv_folds 3
- Faster training for development
- Less reliable but good for rapid iteration

5.2 VALIDATION SPLIT OPTIONS
----------------------------

Standard: --val_size 0.2 (20% validation)
Conservative: --val_size 0.25 (25% validation)
Aggressive: --val_size 0.15 (15% validation)

===============================================================================
6. OUTPUT AND MODEL MANAGEMENT
===============================================================================

6.1 OUTPUT DIRECTORY STRUCTURE
------------------------------
models/
├── lightgbm_[category]_model.txt     # LightGBM model file
├── xgboost_[category]_model.json     # XGBoost model file
├── training_results_[timestamp].json  # Detailed metrics
└── visualizations/                    # Performance plots
    ├── roc_curve_[model]_[category].png
    ├── pr_curve_[model]_[category].png
    ├── confusion_matrix_[model]_[category].png
    ├── feature_importance_[model]_[category].png
    └── model_performance_comparison.png

6.2 RECOMMENDED OUTPUT DIRECTORIES
----------------------------------
- optimal_models/          # Production-ready models
- conservative_models/     # Maximum anti-overfitting
- multi_category_models/   # Multiple file types
- universal_models/        # All categories
- fast_models/            # Quick development models
- category_specific/      # Single category models

===============================================================================
7. TROUBLESHOOTING COMMON ISSUES
===============================================================================

7.1 MEMORY ERRORS
-----------------
Problem: "MemoryError" or system freezing
Solution: Reduce --sample_size parameter
Example: Use 500000 instead of 1000000

7.2 SLOW TRAINING
-----------------
Problem: Training takes too long
Solutions:
- Increase learning rates (0.1 instead of 0.05)
- Reduce n_estimators (1000 instead of 2000)
- Reduce CV folds (3 instead of 5)
- Use smaller sample size

7.3 OVERFITTING DETECTION
-------------------------
Signs of Overfitting:
- Large gap between validation and test AUC (>0.05)
- Perfect validation scores (1.0000)
- Poor cross-validation consistency

Solutions:
- Use conservative parameters (section 2.1)
- Increase regularization
- Reduce model complexity
- Add more training data

7.4 POOR PERFORMANCE
-------------------
Problem: Low AUC scores (<0.90)
Potential Causes:
- Insufficient training data
- Too much regularization
- Learning rate too low
- Early stopping too aggressive

Solutions:
- Increase sample size
- Reduce regularization parameters
- Increase learning rate to 0.08-0.1
- Increase early_stopping_rounds to 100

===============================================================================
8. RECOMMENDED TRAINING WORKFLOW
===============================================================================

8.1 DEVELOPMENT PHASE
---------------------
1. Start with fast training (section 2.2)
2. Experiment with different categories
3. Tune key parameters (learning_rate, regularization)
4. Use 3-fold CV for quick feedback

8.2 VALIDATION PHASE
--------------------
1. Use optimal training (section 1.1)
2. Run 5-fold cross-validation
3. Compare multiple models
4. Validate on hold-out test set

8.3 PRODUCTION PHASE
--------------------
1. Use conservative training (section 2.1)
2. Run 10-fold cross-validation
3. Train on maximum available data
4. Thoroughly validate generalization

===============================================================================
9. PERFORMANCE BENCHMARKS
===============================================================================

Expected Performance Ranges:

STANDARD TRAINING:
Category         | Expected AUC | Training Time | Memory Usage
---------------- | ------------ | ------------- | ------------
Win32            | 0.94-0.97    | 10-15 min     | 8-12 GB
Win64            | 0.92-0.96    | 6-10 min      | 6-8 GB
APK              | 0.90-0.95    | 4-6 min       | 4-6 GB
PDF              | 0.88-0.94    | 2-3 min       | 2-3 GB
ELF              | 0.85-0.93    | 1-2 min       | 1-2 GB
.NET             | 0.91-0.96    | 4-6 min       | 4-6 GB
All Combined     | 0.93-0.96    | 20-30 min     | 12-16 GB

BEST LIGHTGBM TRAINING ⭐:
Category         | Expected AUC | Training Time | Memory Usage
---------------- | ------------ | ------------- | ------------
Win32 Best LGB   | 0.97-0.99    | 15-20 min     | 10-14 GB
Win64 Best LGB   | 0.96-0.98    | 12-15 min     | 8-10 GB
APK Best LGB     | 0.94-0.97    | 8-10 min      | 6-8 GB
PDF Best LGB     | 0.92-0.96    | 4-6 min       | 3-4 GB
ELF Best LGB     | 0.90-0.95    | 3-4 min       | 2-3 GB
.NET Best LGB    | 0.95-0.98    | 8-10 min      | 6-8 GB

BEST XGBOOST TRAINING ⭐:
Category         | Expected AUC | Training Time | Memory Usage
---------------- | ------------ | ------------- | ------------
Win32 Best XGB   | 0.96-0.98    | 18-25 min     | 12-16 GB
Win64 Best XGB   | 0.95-0.97    | 15-20 min     | 10-12 GB
APK Best XGB     | 0.93-0.96    | 10-12 min     | 8-10 GB
PDF Best XGB     | 0.91-0.95    | 6-8 min       | 4-5 GB
ELF Best XGB     | 0.89-0.94    | 4-6 min       | 3-4 GB
.NET Best XGB    | 0.94-0.97    | 10-12 min     | 8-10 GB

Note: Performance varies based on system specifications and parameters used.
⭐ Best configurations optimized for maximum accuracy with proper regularization.

===============================================================================
10. CONTACT AND SUPPORT
===============================================================================

For issues with training commands:
1. Check parameter syntax using: python train_malware_models.py --help
2. Verify dataset paths and file existence
3. Monitor system resources during training
4. Review training logs for error messages

Training Script Location: train_malware_models.py
Dataset Location: ember2024_merged_categories/
Documentation: DATASET_STATUS_REPORT.md

===============================================================================
END OF GUIDE
===============================================================================

Last Updated: August 28, 2025
Version: 1.0
Total Commands Provided: 12 main scenarios + variations
Compatible with: EMBER2024 Ultra-Focused Feature Dataset
